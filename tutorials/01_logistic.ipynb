{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression in pytorch\n",
    "## What's in this tutorial?\n",
    "This notebook will walk you through the basic uses of `mandala` for storing\n",
    "and tracking ML experiment results. It uses logistic regression on a synthetic\n",
    "dataset as a \"minimally interesting\" example of a data management use case. By\n",
    "following this ML mini-project, you will learn how to\n",
    "- break up an experiment into Python functions whose calls can be\n",
    "tracked and queried by `mandala`;\n",
    "- use `mandala`'s memoization to avoid re-running expensive computations and to\n",
    "naturally interact with and grow your project (by adjusting the parameters and/or\n",
    "adding new code);\n",
    "- repurpose the (pure Python) code of your experiments into a *query interface*\n",
    "to their results \"for free\";\n",
    "- modify, or create new versions of, your experimental primitives, and have them\n",
    "  seamlessly interact with the results of previous runs.\n",
    "\n",
    "Ultimatley, the features of `mandala` work together to enable you to evolve\n",
    "complex ML projects by writing only the plain-Python code that you'd write in a\n",
    "temporary in-memory interactive session, yet get the benefits of a\n",
    "database-backed experiment tracking system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# recommended way to import mandala functionality\n",
    "from mandala.imports import *\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.random.set_rng_state(torch.manual_seed(0).get_state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define experiment primitives\n",
    "You'll break the project into two main functions: to generate the synthetic\n",
    "dataset, and to train the model. Below is fairly standard `pytorch` code for\n",
    "these. Note the use of `@op` to mark the functions as tracked by `mandala` -\n",
    "more on that shortly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIMENSION = 10\n",
    "\n",
    "# main `mandala` decorator; like @functools.lru_cache, but with extra functionality.\n",
    "# Currently, you must specify the exact number of inputs (i.e., no *args or **kwargs),\n",
    "# and the number of outputs (using a type annotation with a `Tuple` if there are\n",
    "# multiple outputs).\n",
    "@op\n",
    "def generate_dataset() -> Tuple[TensorDataset, TensorDataset]:\n",
    "    \"\"\"\n",
    "    Generate a simple synthetic dataset for logistic regression, perform a\n",
    "    80/20 train/test split, and return the results as `TensorDataset`s.\n",
    "    \"\"\"\n",
    "    n_samples = 1000\n",
    "    x = np.random.randn(n_samples, DATA_DIMENSION)\n",
    "    y = x[:, 0] > 0\n",
    "    x, y = torch.from_numpy(x).float(), torch.from_numpy(y).long()\n",
    "    train_size = int(0.8 * n_samples)\n",
    "    train_dataset = TensorDataset(x[:train_size], y[:train_size])\n",
    "    test_dataset = TensorDataset(x[train_size:], y[train_size:])\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(DATA_DIMENSION, 2)\n",
    "\n",
    "    def forward(self, feature):\n",
    "        output = self.linear(feature)\n",
    "        return output\n",
    "\n",
    "\n",
    "@op\n",
    "def train_model(\n",
    "    train_dataset: TensorDataset,\n",
    "    test_dataset: TensorDataset,\n",
    "    learning_rate: float = 0.001,\n",
    "    batch_size: int = 100,\n",
    "    num_epochs: int = 3,\n",
    ") -> Tuple[LogisticRegression, float]:\n",
    "    \"\"\"\n",
    "    Train a logistic model on the given training dataset with the given\n",
    "    hyperparameters.\n",
    "\n",
    "    Prints out the train loss and test accuracy at the end of\n",
    "    each epoch. Returns the trained model and the final test accuracy.\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LogisticRegression().to(device)\n",
    "    loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for xs, ys in train_loader:\n",
    "            xs = xs.to(device)\n",
    "            ys = ys.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(xs)\n",
    "            loss_value = loss(output, ys)\n",
    "            loss_value.backward()\n",
    "        optimizer.step()\n",
    "        # test\n",
    "        model.eval()\n",
    "        accurate, total = 0, 0\n",
    "        for xs, ys in test_loader:\n",
    "            xs = xs.to(device)\n",
    "            ys = ys.to(device)\n",
    "            output = model(xs)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += ys.size(0)\n",
    "            accurate += (predicted == ys).sum()\n",
    "        acc = 100 * accurate / total\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Training loss: {round(loss_value.item(), 2)}. Test accuracy: {round(acc.item(), 2)}\"\n",
    "        )\n",
    "    return model, round(float(acc.item()), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Hello world\", or: run the pipeline and store the results\n",
    "Now that you have defined the functions that make up your pipeline, you can\n",
    "run it with the default parameters to see how well the model performs!\n",
    "\n",
    "The `@op` decorator on the functions above tells `mandala` to track the calls to\n",
    "these functions and store their results - but this only happens when you call\n",
    "these functions *in the context of a given `Storage` object*. So go ahead and\n",
    "create a storage for the project: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This storage will hold the results of all the experiments you run in this\n",
    "notebook. Now, run the pipeline and save its results by wrapping the code you'd\n",
    "normally write in a `storage.run()` context manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    model, acc = train_model(train_dataset, test_dataset)\n",
    "    print(f\"Final accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "A lot happened behind the scenes in these few lines of code! Let's break it\n",
    "down:\n",
    "- Inside the `storage.run()` block, each time an `@op`-decorated function is\n",
    "called **for the first time** on a set of inputs, `mandala` stores the inputs\n",
    "and outputs of this call in the storage. \n",
    "- Values shared between calls are stored only once. So\n",
    "  `train_dataset` will appear in storage as both the output to the call to\n",
    "  `generate_dataset`, and the input to the call to `train_model`.\n",
    "- The `acc` object (like all objects returned by `@op`-decorated functions) is a\n",
    "*value reference*, which is a value wrapped with storage-related metadata. \n",
    "\n",
    "So, what happens when you call `@op`-decorated functions *a second time* on the\n",
    "same inputs? Find out by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    model, acc = train_model(train_dataset, test_dataset)\n",
    "    print(f\"Final accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that this time the intermediate training results did not get printed!**.\n",
    "This is because `mandala` recognized that the inputs to the functions were the\n",
    "same as before, and so it didn't need to re-run the calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what?\n",
    "This was the simplest non-trivial use case of `mandala`! However, at this point\n",
    "it is just a glorified `pickle`-based memoization system. Its real power comes\n",
    "from the way in which `mandala`'s memoization *composes* with the rest of the\n",
    "Python language, which allow you to manage complex experiments with the minimal\n",
    "amount of plain-Python code, as we'll see next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grow the project with new parameters\n",
    "Running the pipeline once is nice, but where `mandala` really shines is in\n",
    "enabling you to grow a computational project in various ways with the minimal\n",
    "necessary code changes, and have the storage interfaces \"just work\". \n",
    "\n",
    "Let's begin exploring this by investigating the effect of changing the learning\n",
    "rate of the model. So far, you have been using the default learning rate of\n",
    "`0.001`. Let's try a few other values, but also see how they compare with the\n",
    "default value. Thanks to memoization, this is easy to do without re-doing\n",
    "expensive work: we can use a list of values for the `learning_rate` parameter\n",
    "that includes the default, and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    for learning_rate in [0.001, 0.01, 0.1]:\n",
    "        model, acc = train_model(train_dataset, test_dataset, learning_rate)\n",
    "        print(\n",
    "            # `unwrap()` is used to get the value wrapped by a `ValueRef`\n",
    "            f\"===end of run=== learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first run was re-used from before, while the 2nd and 3rd were\n",
    "freshly computed. We see that the higher the learning rate, the better the\n",
    "final accuracy. Now, let's try varying the batch size as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for learning_rate in [0.001, 0.01, 0.1]:\n",
    "            model, acc = train_model(\n",
    "                train_dataset, test_dataset, learning_rate, batch_size\n",
    "            )\n",
    "            print(\n",
    "                f\"===end of run=== batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the results\n",
    "By now, you have run the pipeline with many different combinations of\n",
    "parameters, and it's getting difficult to make sense of all the results so far.\n",
    "One option to \"query\" the results is to just re-run the above workflow, or a\n",
    "\"sub-workflow\" of it. \n",
    "\n",
    "For example, how might you get all the results for a given learning rate, e.g.\n",
    "`learning_rate=0.1`? One answer: just by re-running the subset of the above code\n",
    "using this value of the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for learning_rate in [0.1]:  # only change relative to previous cell\n",
    "            model, acc = train_model(\n",
    "                train_dataset, test_dataset, learning_rate, batch_size\n",
    "            )\n",
    "            print(\n",
    "                f\"===end of run=== batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of storage access pattern is called **retracing**: you \"retrace\"\n",
    "computational code that you have already run in order to recover the quantities\n",
    "computed along the way. You can use retracing to query existing results (like\n",
    "you did above), or to easily add new parameters/logic that need to compute over\n",
    "existing results.\n",
    "\n",
    "However, sometimes you don't have a specific piece of code to retrace and just\n",
    "want to look at all the results in storage. For this, you can use a \"full\n",
    "storage search\" query interface. One option for this is to directly point to\n",
    "variables in the computation. For example, by referring to the `acc` variable\n",
    "from the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.similar(acc, context=True) # this may take a few seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened? ðŸ¤¯\n",
    "Behind the scenes, `mandala` builds a computational graph that links the inputs\n",
    "and outputs to each call (as well as elements of collections to the collection\n",
    "itself, but this is a topic for another tutorial :) ). This means that the last\n",
    "value of `acc` in the loop above \"knows\" that it was computed by a certain\n",
    "composition of memoized functions. \n",
    "\n",
    "This composition serves as the \"shape\" of the query, which looks for\n",
    "computations in the storage that follow this same pattern, but possibly with\n",
    "different values. \n",
    "\n",
    "More formally,\n",
    "- **variables** stand for a value stored in storage. These include the `Q()`\n",
    "placeholder objects in the graph printed out above, as well as the values\n",
    "returned by `@op`-decorated functions in the `storage.query()` block.\n",
    "- **constraints** (i.e. function calls) enforce functional relationships between\n",
    "variables.\n",
    "\n",
    "The result of the query is a table, where each row is a choice of values for \n",
    "the variables that satisfy **all** the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to limit the columns of the table, just omit the `context=True`\n",
    "argument, which will restrict to only the variables you pass in explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.similar(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you have the option of running or editing the query manually by\n",
    "copy-pasting the graph into a `with storage.query()` context. For example, you\n",
    "can give human-readable names to the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.query() as q:\n",
    "    batch_size = Q() # input to computation; can match anything\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    learning_rate = Q() # input to computation; can match anything\n",
    "    num_epochs = Q() # input to computation; can match anything\n",
    "    _, acc = train_model(train_dataset=train_dataset, test_dataset=test_dataset, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)\n",
    "storage.df(batch_size, learning_rate, num_epochs, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a larger dataset, or: how to modify memoized functions gracefully\n",
    "A very common scenario in ML is to extend an existing experimental primitive\n",
    "with new functionality, for example by exposing a hard-coded parameter, or\n",
    "adding a new behavior to the function (e.g., an option to use a different\n",
    "algorithm). With `mandala` you can seamlessly integrate this new functionality\n",
    "with the results of previous runs.\n",
    "\n",
    "For example, suppose you want to improve the accuracy numbers above. So far, you\n",
    "have been running all models on a synthetic dataset of size `1000`. Let's try\n",
    "increasing the dataset size while still sampling from the same distribution and\n",
    "see if that helps! \n",
    "\n",
    "To do this, just directly modify the `generate_dataset` function\n",
    "to take an additional argument `n_samples` with a default value of `1000`. Let's\n",
    "also print out a message when it's called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@op\n",
    "def generate_dataset(n_samples: int = 1000) -> Tuple[TensorDataset, TensorDataset]:\n",
    "    print(\"Hi from `generate_dataset` with a new argument!\")\n",
    "    x = np.random.randn(n_samples, DATA_DIMENSION)\n",
    "    y = x[:, 0] > 0\n",
    "    x, y = torch.from_numpy(x).float(), torch.from_numpy(y).long()\n",
    "    train_size = int(0.8 * n_samples)\n",
    "    train_dataset = TensorDataset(x[:train_size], y[:train_size])\n",
    "    test_dataset = TensorDataset(x[train_size:], y[train_size:])\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to the memoized calls after we change the function? Find out by\n",
    "re-running the project so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset()\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for learning_rate in [0.001, 0.01, 0.1]:\n",
    "            model, acc = train_model(\n",
    "                train_dataset, test_dataset, learning_rate, batch_size\n",
    "            )\n",
    "            print(\n",
    "                f\"===end of run=== batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `generate_dataset` was quiet, indicating that it was not re-run!\n",
    "This is because `mandala` recognized that the value of `n_samples` was the\n",
    "default, so it re-used the call to the previous (zero-argument)\n",
    "`generate_dataset`. This is how adding new inputs to memoized functions works:\n",
    "the default value (which must always be provided for new inputs) is used to\n",
    "distinguish between old and new calls. You can add as many new inputs as you\n",
    "want, as long as you provide a default value for each of them.\n",
    "\n",
    "Let's finally run the pipeline with a larger dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    for n_samples in [1000, 2000]:\n",
    "        train_dataset, test_dataset = generate_dataset(n_samples=n_samples)\n",
    "        for batch_size in [100, 200, 400]:\n",
    "            for learning_rate in [0.001, 0.01, 0.1]:\n",
    "                model, acc = train_model(\n",
    "                    train_dataset, test_dataset, learning_rate, batch_size\n",
    "                )\n",
    "                print(\n",
    "                    f\"===end of run=== n_samples: {n_samples}, batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get a nice table with the results; you can do this by minimally\n",
    "modifying the query code from before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.query() as q:\n",
    "    batch_size = Q() # input to computation; can match anything\n",
    "    n_samples = Q() # input to computation; can match anything\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples=n_samples)\n",
    "    learning_rate = Q() # input to computation; can match anything\n",
    "    num_epochs = Q() # input to computation; can match anything\n",
    "    _, acc = train_model(train_dataset=train_dataset, test_dataset=test_dataset, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)\n",
    "storage.df(batch_size, learning_rate, num_epochs, n_samples, acc).sort_values(by='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives you a very simple way to co-evolve your computational code and the\n",
    "query \"interface\" to its results!\n",
    "\n",
    "However, it looks like the larger dataset didn't help! Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Oh no, there's a bug!\", or: function versioning\n",
    "If you look at the `train_model` function carefully, you'll see there's a\n",
    "subtle but crucial mistake: because the call to `optimizer.step()` happens\n",
    "outside the loop over the batches, the model is only updated once per epoch with\n",
    "the last batch!\n",
    "\n",
    "Unfortunately, we have already ran a lot of experiments with this bug. How can\n",
    "we fix this without messing up our storage and re-running more than we need to?\n",
    "Starting the whole project from scratch is not great, because it means we would\n",
    "have to regenerate the datasets too. \n",
    "\n",
    "Fortunately, `mandala` has a simple way to deal with this situation with the\n",
    "minimal possible amount of code changes. You must do two things:\n",
    "- change the function to fix the bug, and **increment the\n",
    "  version number**. By default, each function starts at version `0`. \n",
    "- re-run the experiments that used the old version of the function. All results\n",
    "  that do not depend on the bug will be reused, and only the results that\n",
    "  depend on values that the bugfix changes will be recomputed.\n",
    "\n",
    "**NOTE**: there is an optional, more fine-grained versioning system that \n",
    "automatically tracks the dependencies of each memoized call for changes, and\n",
    "allows you to effectively do the same as above (and much more), described in the\n",
    "next tutorial.\n",
    "\n",
    "So, fix the bug and increment the version number in the `@op` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@op(version=1)\n",
    "def train_model(\n",
    "    train_dataset: TensorDataset,\n",
    "    test_dataset: TensorDataset,\n",
    "    learning_rate: float = 0.001,\n",
    "    batch_size: int = 100,\n",
    "    num_epochs: int = 3,\n",
    ") -> Tuple[LogisticRegression, float]:\n",
    "    \"\"\"\n",
    "    Train a logistic model on the given training dataset with the given\n",
    "    hyperparameters.\n",
    "\n",
    "    Prints out the train loss and test accuracy at the end of\n",
    "    each epoch. Returns the trained model and the final test accuracy.\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LogisticRegression().to(device)\n",
    "    loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        for xs, ys in train_loader:\n",
    "            xs = xs.to(device)\n",
    "            ys = ys.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(xs)\n",
    "            loss_value = loss(output, ys)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()  #! bugfix here\n",
    "        # test\n",
    "        model.eval()\n",
    "        accurate, total = 0, 0\n",
    "        for xs, ys in test_loader:\n",
    "            xs = xs.to(device)\n",
    "            ys = ys.to(device)\n",
    "            output = model(xs)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += ys.size(0)\n",
    "            accurate += (predicted == ys).sum()\n",
    "        acc = 100 * accurate / total\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Training loss: {round(loss_value.item(), 2)}. Test accuracy: {round(acc.item(), 2)}\"\n",
    "        )\n",
    "    return model, round(float(acc.item()), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, literally copy-paste the computation + query code from above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    for n_samples in [1000, 2000]:\n",
    "        train_dataset, test_dataset = generate_dataset(n_samples=n_samples)\n",
    "        for batch_size in [100, 200, 400]:\n",
    "            for learning_rate in [0.001, 0.01, 0.1]:\n",
    "                model, acc = train_model(\n",
    "                    train_dataset, test_dataset, learning_rate, batch_size\n",
    "                )\n",
    "                print(\n",
    "                    f\"===end of run=== n_samples: {n_samples}, batch_size: {batch_size}, learning_rate: {learning_rate}, acc: {round(unwrap(acc), 2)}\"\n",
    "                )\n",
    "\n",
    "with storage.query() as q:\n",
    "    batch_size = Q() # input to computation; can match anything\n",
    "    n_samples = Q() # input to computation; can match anything\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples=n_samples)\n",
    "    learning_rate = Q() # input to computation; can match anything\n",
    "    num_epochs = Q() # input to computation; can match anything\n",
    "    _, acc = train_model(train_dataset=train_dataset, test_dataset=test_dataset, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)\n",
    "storage.df(batch_size, learning_rate, num_epochs, n_samples, acc).sort_values(by='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! Now we have the correct results. Maybe. At least, they make more sense than\n",
    "before: accuracy went up, and the larger dataset actually helped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model weights, or: track workflows with complex logic\n",
    "So far, you have been running the same two-part pipeline independently for\n",
    "several sets of parameters. This kind of workflow is common in ML projects, and\n",
    "many libraries provide things like hyperparameter tuning interfaces for such\n",
    "cases (e.g., `sklearn`). \n",
    "\n",
    "However, real-world projects often go beyond these \"embarrassingly DAG-like\"\n",
    "workflows, and require more complex logic, which in turn leads to more complex\n",
    "patterns for computation reuse and data management. \n",
    "\n",
    "This is another place where `mandala` shines: thanks to the transparent way you\n",
    "build experiments directly in plain Python, you are free to make the logic as\n",
    "complex as necessary, and still get a simple way to access the results.\n",
    "\n",
    "To illustrate this, let's compare the trained model weights of models trained\n",
    "with the same batch size and dataset, but different learning rates. First,\n",
    "define a function that computes the distance between the weights of two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@op\n",
    "def get_model_distance(\n",
    "    model_1: LogisticRegression,\n",
    "    model_2: LogisticRegression,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the distance between two models' weights\n",
    "    \"\"\"\n",
    "    weight_distance = torch.dist(model_1.linear.weight, model_2.linear.weight)\n",
    "    bias_distance = torch.dist(model_1.linear.bias, model_2.linear.bias)\n",
    "    return round(float(weight_distance + bias_distance), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare the mistakes of all the models trained on the synthetic dataset of\n",
    "size `2000` for each batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples=2_000)\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for lr_1 in [0.001, 0.01, 0.1]:\n",
    "            for lr_2 in [0.001, 0.01, 0.1]:\n",
    "                if lr_1 <= lr_2:\n",
    "                    continue\n",
    "                model_1, acc_1 = train_model(\n",
    "                    train_dataset, test_dataset, lr_1, batch_size\n",
    "                )\n",
    "                model_2, acc_2 = train_model(\n",
    "                    train_dataset, test_dataset, lr_2, batch_size\n",
    "                )\n",
    "                model_dist = get_model_distance(model_1, model_2)\n",
    "                print(\n",
    "                    f\"===end of run=== batch_size: {batch_size}, lr_1: {lr_1}, lr_2: {lr_2}, overlap coefficient: {unwrap(model_dist)}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, note that thanks to the memoization, the only new computations here\n",
    "were calls to the new `compare_mistakes` function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the new workflow\n",
    "How might you get a nice table with the results of the above workflow? Again,\n",
    "you have two options. \n",
    "\n",
    "The simplest is just to retrace the code that you have already run, and collect \n",
    "the results as rows of a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.run():\n",
    "    rows = []\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples=2_000)\n",
    "    for batch_size in [100, 200, 400]:\n",
    "        for lr_1 in [0.001, 0.01, 0.1]:\n",
    "            for lr_2 in [0.001, 0.01, 0.1]:\n",
    "                if lr_1 <= lr_2:\n",
    "                    continue\n",
    "                model_1, acc_1 = train_model(\n",
    "                    train_dataset, test_dataset, lr_1, batch_size\n",
    "                )\n",
    "                model_2, acc_2 = train_model(\n",
    "                    train_dataset, test_dataset, lr_2, batch_size\n",
    "                )\n",
    "                model_distance = get_model_distance(model_1, model_2)\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"lr_1\": lr_1,\n",
    "                        \"lr_2\": lr_2,\n",
    "                        \"model_distance\": unwrap(model_distance),\n",
    "                    }\n",
    "                )\n",
    "df = pd.DataFrame(rows)\n",
    "df.sort_values(by=[\"model_distance\"], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also use the `storage.query()` context manager, but you\n",
    "need to be careful to specify the correct constraints! In particular, you must\n",
    "create two placeholders for the learning rate (why?). \n",
    "\n",
    "The easiest way to build the query structure is to copy-paste the computational\n",
    "code, and then modify it to replace iteration over a parameter range with a\n",
    "placeholder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with storage.query():\n",
    "    n_samples = Q().named(\"n_samples\")\n",
    "    train_dataset, test_dataset = generate_dataset(n_samples=n_samples)\n",
    "    batch_size = Q().named(\"batch_size\")\n",
    "    lr_1 = Q().named(\"lr_1\")\n",
    "    lr_2 = Q().named(\"lr_2\")\n",
    "    model_1, acc_1 = train_model(train_dataset, test_dataset, lr_1, batch_size)\n",
    "    model_2, acc_2 = train_model(train_dataset, test_dataset, lr_2, batch_size)\n",
    "    model_distance = get_model_distance(model_1, model_2)\n",
    "df = storage.df(batch_size, lr_1, lr_2, model_distance.named(\"model_distance\"))\n",
    "\n",
    "df.sort_values(by=[\"model_distance\"], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that more similar learning rates often (but not always) lead to a\n",
    "smaller distance between the corresponding learned models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The project you just finished illustrated all the main features of\n",
    "`mandala`, and how they work together to enable a very simple and direct\n",
    "way of building and querying computational projects. While large-scale ML \n",
    "projects may require some additional patterns for properly using `mandala`, the\n",
    "patterns you have already seen can take you a long way.\n",
    "\n",
    "### The importance of a good function decomposition\n",
    "Finally, it's a good time to reflect on something that often gets overlooked in\n",
    "ML, but is especially important when it comes to effective data management with\n",
    "`mandala`: **function decomposition**. \n",
    "\n",
    "Decomposing a project like the above into functions (i.e., `generate_dataset`,\n",
    "`train_model`, `get_model_distance`) is something you often find yourself doing\n",
    "almost without thinking. It's a necessary practice for managing complexity in\n",
    "software engineering and ML experimentation in general. \n",
    "\n",
    "However, it is also particularly important to find a \"good\" decomposition when\n",
    "tracking experiments with `mandala`, because the kind of decomposition you use\n",
    "has consequences on the performance of your computations and storage system.\n",
    "\n",
    "For example, consider an alternative decomposition with a single function that\n",
    "does both the dataset generation and the model training:\n",
    "```python\n",
    "def generate_and_train(n_samples, batch_size, learning_rate) -> Tuple[LogisticRegression, float]:\n",
    "    ...\n",
    "```\n",
    "\n",
    "A frequent use case is to train many models on the same dataset, but with\n",
    "different hyperparameters. With a single function, you'd have to re-run the\n",
    "dataset generation function every time, even though the dataset is the same!\n",
    "\n",
    "This illustates the main principle of good function decomposition:\n",
    "**functions should be as small as possible, but no smaller**. It's best to pack\n",
    "independent units of work that you may want to combine in different ways into\n",
    "separate functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30c0510467e0bc33a523a84a8acb20ce0730b8eb0ee254a4b0039140f094f217"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
