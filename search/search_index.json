{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This is the documentation for mandala, a simple &amp; elegant experiment tracking framework for Python.</p> <p>Most methods in <code>mandala</code> are provided by the <code>Storage</code> and <code>ComputationFrame</code> classes. In general, you'll probably find yourself only interacting with 5-10 methods on a regular basis, and their docstrings provide detailed explanations. </p> <p>To complement this, this documentation contains a few short walkthroughs illustrating the use of these methods.</p>"},{"location":"blog/01_cf/","title":"Tidy Computations","text":"<p>In data-driven fields, such as machine learning, a lot of effort is spent organizing computational data \u2014 results of running programs \u2014 so that it can be analyzed and manipulated. This blog post introduces the <code>ComputationFrame</code> (CF) data structure \u2014 a synthesis of computation graphs and relational databases \u2014 which provides a natural and simple grammar of operations to eliminate this effort.</p> <p>The main benefit of CFs is that they give a single view of heterogeneous computations that go beyond a fixed schema. They automatically represent in a familiar and intuitive way constructs like conditional execution, feedback loops, branching/merging pipelines, and aggregation/indexing using collections. This view can be declaratively queried for relationships between any variables in (literally) a single line of code, without leaving Python or writing in a domain-specific language like SQL.</p> <p><code>ComputationFrame</code> is implemented as part of mandala, a Python library for experiment tracking and incremental computation. All examples in this post are ran using the library.</p>"},{"location":"blog/01_cf/#so-whats-a-computationframe","title":"So what's a <code>ComputationFrame</code>?","text":"<p>A <code>ComputationFrame</code> is a \"generalized dataframe\", where the set of columns is replaced by a computation graph of variables and operations, and rows are (possibly partial) executions of the graph.</p>"},{"location":"blog/01_cf/#minimal-interesting-example","title":"Minimal interesting example","text":"<p>Below is a simple example of defining two Python functions  (decorated via <code>@op</code> from <code>mandala</code>), running some computations using them, and creating a CF from the results.</p> <pre><code># for Google Colab\ntry:\n    import google.colab\n    !pip install git+https://github.com/amakelov/mandala\nexcept:\n    pass\n\nfrom mandala.imports import *\n\n@op(output_names=['y'])\ndef increment(x): return x + 1\n\n@op(output_names=['w'])\ndef add(y, z): return y + z\n\n# compose the operations\nwith Storage() as storage: # the `storage` automatically stores calls to `@op`s\n    for x in range(5):\n        y = increment(x)\n        if x % 2 == 0:\n            w = add(y=y, z=x)\n\n# get a CF for just the `increment` operation \ncf = storage.cf(increment)\n# expand the CF to include the `add` operation\ncf = cf.expand_forward()\n# draw the CF\ncf.draw(verbose=True, orientation='LR')\n# convert the CF to a dataframe\nprint(cf.df().to_markdown())\n</code></pre> <p></p> <pre><code>|    |   x | increment                   |   y | add                   |   w |\n|---:|----:|:----------------------------|----:|:----------------------|----:|\n|  0 |   2 | Call(increment, hid=5dd...) |   3 | Call(add, hid=626...) |   5 |\n|  1 |   4 | Call(increment, hid=adf...) |   5 | Call(add, hid=deb...) |   9 |\n|  2 |   3 | Call(increment, hid=df2...) |   4 |                       | nan |\n|  3 |   0 | Call(increment, hid=230...) |   1 | Call(add, hid=247...) |   1 |\n|  4 |   1 | Call(increment, hid=6e2...) |   2 |                       | nan |\n</code></pre> <p>This small example illustrates the main components of the CF workflow:</p> <ul> <li>run some computations: we ran a computation by composing calls to <code>increment</code> and <code>add</code> (which are automatically saved by <code>mandala</code> in a way that  keeps track of how the calls compose).</li> <li>create a CF and add desired context to it: we explored the computation by starting from a CF containing all calls to <code>increment</code> and expanding it forward to add the downstream calls to <code>add</code>.</li> <li>convert to a dataframe for downstream analysis: we turned the expanded CF into a dataframe, where the columns are the variables and operations in the graph, and the rows are the executions of the graph \u2014 recorded as values of variables and <code>Call</code> objects for operations. </li> </ul> <p>When a given computation is partial, missing values and calls are represented by nulls in the dataframe, which allows for easy filtering and analysis.</p>"},{"location":"blog/01_cf/#precise-definition","title":"Precise definition","text":"<p>Concretely, a <code>ComputationFrame</code> consists of:</p> <ul> <li>a directed computation graph of operations and variables, where each operation has multiple named inputs and outputs. The names of these inputs and outputs are the edge labels in the graph;</li> <li>a set of calls for each operation node, and a set of values for each variable node. Details about how calls/values are distinguished are beyond the scope of this post, but the key idea is that each call/value has a unique ID based on its full computational history in terms of composition of <code>@op</code> calls; see here for more.</li> </ul> <p>The only \u2014 but key \u2014 property this data must satisfy is that, whenever input/output variables are connected to an operation node <code>f</code>, the corresponding input/output values of all calls in <code>f</code> are found in these variables.</p>"},{"location":"blog/01_cf/#case-studies","title":"Case studies","text":"<p>To illustrate the versatility of <code>ComputationFrame</code>s and the grammar of operations they support, let's now consider a fairly standard machine learning pipeline as a running example. </p> <p>The goal will be to train several kinds of machine learning models on the moons dataset from scikit-learn, and iterate on the pipeline by adding new preprocessing steps, models, hyperparameters and ensembling across model types. Here's some code to get us started:</p> <pre><code>from sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nnp.random.seed(42)\n\n@op(output_names=[\"X\", \"y\"])\ndef get_data():\n    return make_moons(n_samples=1000, noise=0.3, random_state=42)\n\n@op(output_names=[\"X_train\", \"X_test\", \"y_train\", \"y_test\"])\ndef get_train_test_split(X, y):\n    return tuple(train_test_split(X, y, test_size=0.2, random_state=42))\n\n@op(output_names=[\"X_scaled\"])\ndef scale_data(X):\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    return X\n</code></pre> <p>We begin by loading the dataset, optionally scaling it, and splitting it into training and test sets:</p> <pre><code>with Storage() as storage:\n    for scale in (True, False):\n        X, y = get_data()\n        if scale:\n            X = scale_data(X=X)\n        X_train, X_test, y_train, y_test = get_train_test_split(X=X, y=y)\n</code></pre>"},{"location":"blog/01_cf/#in-place-updates-as-cycles-in-the-computation-graph","title":"In-place updates as cycles in the computation graph","text":"<p>When a variable in the program is updated in-place (like how <code>X</code> is scaled in the above code) or feeds back on itself in a more complex way, it's natural to represent this as a cycle in the high-level computation graph. </p> <p>The CF expansion algorithm notices these cases by default. When we expand a CF back starting from <code>get_train_test_split</code>, we see two values for <code>X</code> used as inputs. When we keep expanding, we see that one of these values was actually obtained from the other by applying the <code>scale_data</code> operation, so we add a  cycle to the graph:</p> <pre><code>cf = storage.cf(get_train_test_split).expand_back(recursive=True)\ncf.draw(verbose=True, orientation='TB')\n</code></pre> <p></p> <p>Note that the underlying low-level call graph that the CF represents is still acyclic, because we distinguish between the value before and after the update. This shows up in the dataframe representation as two rows, where only one of the rows contains a call to <code>scale_data</code>:</p> <pre><code>print(cf.df()[['get_data', 'scale_data', 'get_train_test_split']].to_markdown())\n</code></pre> <pre><code>|    | get_data                   | scale_data                   | get_train_test_split                   |\n|---:|:---------------------------|:-----------------------------|:---------------------------------------|\n|  0 | Call(get_data, hid=73a...) |                              | Call(get_train_test_split, hid=7be...) |\n|  1 | Call(get_data, hid=73a...) | Call(scale_data, hid=d6b...) | Call(get_train_test_split, hid=e0a...) |\n</code></pre>"},{"location":"blog/01_cf/#pipelines-that-branch-andor-merge","title":"Pipelines that branch and/or merge","text":"<p>Next, let's train and evaluate two kinds of models: an SVC (support vector classifier) and a random forest. We then initialize a CF from the <code>eval_model</code> function, and we call <code>expand_back</code> to trace back the full computation graph:</p> <pre><code>from sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n@op(output_names=[\"svc_model\"])\ndef train_svc(X_train, y_train, C: float = 1.0, kernel: str = \"linear\"):\n    model = SVC(C=C, kernel=kernel)\n    model.fit(X_train, y_train)\n    return model\n\n@op(output_names=[\"rf_model\"])\ndef train_random_forest(X_train, y_train, n_estimators: int = 5, max_depth: int = 5):\n    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n    model.fit(X_train, y_train)\n    return model\n\n@op(output_names=[\"accuracy\",])\ndef eval_model(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    return acc\n\nwith storage:\n    for scale in (True, False):\n        X, y = get_data()\n        if scale:\n            X = scale_data(X=X)\n        ### new: training an SVC and a Random Forest\n        X_train, X_test, y_train, y_test = get_train_test_split(X=X, y=y)\n        svc_model = train_svc(X_train=X_train, y_train=y_train)\n        svc_acc = eval_model(model=svc_model, X_test=X_test, y_test=y_test)\n        rf_model = train_random_forest(X_train=X_train, y_train=y_train)\n        rf_acc = eval_model(model=rf_model, X_test=X_test, y_test=y_test)\n\ncf = storage.cf(eval_model).expand_back(recursive=True)\ncf.draw(verbose=True)\n</code></pre> <p></p> <p>It's quite clear by looking at the CF's drawing what computations we did. As before, we can obtain a single dataframe that expresses the full computation,  and we can select operations/variables of interest to analyze the results; for example, we see that random forest generally performs better than SVC on this dataset.</p> <pre><code>print(cf.df()[['accuracy', 'scale_data', 'train_svc', 'train_random_forest']].sort_values('accuracy', ascending=False).to_markdown())\n</code></pre> <pre><code>|    |   accuracy | scale_data                   | train_svc                   | train_random_forest                   |\n|---:|-----------:|:-----------------------------|:----------------------------|:--------------------------------------|\n|  3 |      0.915 | Call(scale_data, hid=d6b...) |                             | Call(train_random_forest, hid=c42...) |\n|  2 |      0.885 |                              |                             | Call(train_random_forest, hid=997...) |\n|  0 |      0.82  |                              | Call(train_svc, hid=6a0...) |                                       |\n|  1 |      0.82  | Call(scale_data, hid=d6b...) | Call(train_svc, hid=7d9...) |                                       |\n</code></pre> <p>So is this the full story of this dataset? We might want to investigate further by </p> <ul> <li>training more SVC and random forest models with different hyperparameters;</li> <li>ensembling different types of models to see if this improves performance.</li> </ul> <p>We do this below by trying out more kernels for SVC and increasing the number of trees in the random forest. Importantly, when defining the new <code>eval_ensemble</code> operation, we type-annotate the <code>models</code> input using the custom <code>MList</code> type constructor. This tells <code>mandala</code> to track each element of <code>models</code> as an individual entity in the computation graph.</p> <pre><code>from typing import Any\n\n@op(output_names=[\"accuracy\"])\ndef eval_ensemble(models: MList[Any], X_test, y_test):\n    y_preds = [model.predict(X_test) for model in models]\n    y_pred = np.mean(y_preds, axis=0) &gt; 0.5\n    acc = accuracy_score(y_test, y_pred)\n    return acc\n\nwith storage:\n    for scale in (True, False):\n        X, y = get_data()\n        if scale:\n            X = scale_data(X=X)\n        X_train, X_test, y_train, y_test = get_train_test_split(X=X, y=y)\n        svc_models = []\n        for kernel in ('linear', 'rbf', 'poly'): # new: trying different kernels\n            svc_model = train_svc(X_train=X_train, y_train=y_train, kernel=kernel)\n            svc_acc = eval_model(model=svc_model, X_test=X_test, y_test=y_test)\n            svc_models.append(svc_model)\n        rf_models = []\n        for n_estimators in (5, 10, 20): # new: trying different numbers of estimators\n            rf_model = train_random_forest(X_train=X_train, y_train=y_train, n_estimators=n_estimators)\n            rf_acc = eval_model(model=rf_model, X_test=X_test, y_test=y_test)\n            rf_models.append(rf_model)\n\n        ### new: ensembling\n        ensemble_acc = eval_ensemble(models=svc_models + rf_models, X_test=X_test, y_test=y_test)\n</code></pre>"},{"location":"blog/01_cf/#tidy-tools-merging-computationframes","title":"Tidy tools: merging <code>ComputationFrame</code>s","text":"<p>To illustrate how CFs admit tidy tools \u2014 operations that take as input computation frames and produce new ones \u2014 we'll use the <code>|</code> operator to merge the CFs for <code>eval_model</code> and <code>eval_ensemble</code>. </p> <p>Merging is similar to concatenation of dataframes: it takes the union of the sets of values/calls at each node by matching the names of the operations and variables between the two CFs. Because both our functions use the same names for analogous inputs (<code>X_test</code> and <code>y_test</code>) and outputs (<code>accuracy</code>), this works out nicely: </p> <pre><code>cf = storage.cf(eval_ensemble) | storage.cf(eval_model)\ncf.draw(verbose=True)\n</code></pre> <p></p>"},{"location":"blog/01_cf/#operations-that-aggregate-multiple-results","title":"Operations that aggregate multiple results","text":"<p>As before, let's expand this CF back to see the full computation graph that leads to it:</p> <pre><code>cf = cf.expand_back(recursive=True); cf.draw(verbose=True)\n</code></pre> <p></p> <p>Note the built-in <code>__make_list__</code> operation which got automatically added to the graph. This operation groups the items of the <code>models</code> list passed to <code>eval_ensemble</code> into a single object, which is what's actually passed to  <code>eval_ensemble</code> in the expanded graph. <code>__make_list__</code> is implemented the same as any other operation, with (roughly) the following semantics: <pre><code>@op\ndef __make_list__(*elts:Any) -&gt; list:\n    return list(elts)\n</code></pre> The \"trick\" is that all the variadic inputs <code>elts_0</code>, <code>elts_1</code>, ... are grouped under the <code>*elts</code>-labeled edge, and are found in the <code>model</code> variable. This is  how CFs express the aggregation of multiple values into a single one.</p> <p>How does <code>.df()</code> behave in the presence of such aggregation? We can take a look by selecting enough columns to paint a complete picture of each branch of the overall computation:</p> <pre><code>print(cf.df()[['n_estimators', 'kernel', 'accuracy', ]].sort_values('accuracy', ascending=False).to_markdown())\n</code></pre> <pre><code>|    | n_estimators                 | kernel                                     |   accuracy |\n|---:|:-----------------------------|:-------------------------------------------|-----------:|\n|  1 |                              | rbf                                        |      0.915 |\n|  4 | 5                            |                                            |      0.915 |\n|  3 |                              | rbf                                        |      0.91  |\n|  0 | 20                           |                                            |      0.9   |\n|  2 | 10                           |                                            |      0.9   |\n|  6 | 20                           |                                            |      0.9   |\n|  8 | 10                           |                                            |      0.9   |\n|  7 | ValueCollection([20, 10, 5]) | ValueCollection(['linear', 'rbf', 'poly']) |      0.895 |\n|  9 | ValueCollection([20, 10, 5]) | ValueCollection(['linear', 'rbf', 'poly']) |      0.895 |\n| 12 | 5                            |                                            |      0.885 |\n| 10 |                              | poly                                       |      0.835 |\n|  5 |                              | linear                                     |      0.82  |\n| 11 |                              | linear                                     |      0.82  |\n| 13 |                              | poly                                       |      0.82  |\n</code></pre> <p>Columns where <code>n_estimators</code> is not null correspond to the random forest runs, and similarly columns where <code>kernel</code> is not null correspond to the SVC runs.</p> <p>But now we also get some columns containing a <code>ValueCollection</code> object, which is the CF's way of distinguishing between depending on a single value of a variable versus depending on a collection of values. This makes sense: the <code>eval_ensemble</code> calls depend on multiple models, and in turn, on multiple values in the <code>n_estimators</code> and <code>kernel</code> variables.</p>"},{"location":"blog/01_cf/#conclusion-limitations-outlook","title":"Conclusion, limitations &amp; outlook","text":"<p>The CF visualizations are now getting out of hand, so it's a good time to wrap up! We've seen how <code>ComputationFrame</code>s make complex computations \"self-organizing\", and how they provide a simple grammar of operations to manipulate and analyze them. However, there is much other work in progress on natural CF methods that we haven't touched on:</p> <ul> <li>ways to efficiently select and filter the data in the CF before converting it to a dataframe;</li> <li>operations for fine-grained control over the expansion of the CF, such as ways to explicitly merge/split nodes in the result, or options to expand only up to a certain depth, or whether/when to group calls to the same operation in a single node.</li> </ul> <p>Another limitation is that, currently, <code>ComputationFrame</code>s are not optimized for large-scale computation graphs.</p> <p>There's also much more to <code>mandala</code> than what we've covered here. For example, the incremental computation features that <code>mandala</code> provides, which only recompute <code>@op</code>s when the inputs and/or relevant code changes. If you're interested in learning more, check out the mandala documentation, and feel free to reach out on Twitter.</p>"},{"location":"blog/01_cf/#why-tidy-and-some-other-related-work","title":"Why \"tidy\"? And some other related work","text":""},{"location":"blog/01_cf/#tidy-data-vs-tidy-computations","title":"Tidy data vs tidy computations","text":"<p>In many ways, the ideas here are a re-imagining of Hadley Wickham's Tidy Data in the context of computational data management. In particular, the focus is on computations built from repeated calls to the same set of functions composed in various ways, which is a common pattern in machine learning and scientific computing. In analogy with the tidy data philosophy, the goal of \"tidy computations\" is to eliminate the code and effort required to organize computations, so that only the code to compute and analyze the results remains.</p> <p>Despite the different setups \u2014 data cleaning versus computation tracking \u2014 there are many similarities between the two approaches. This is because in an abstract sense you can think of \"data\" as a kind of \"computation\" that nature has performed via some \"data-generating process\". The difference stems from this process typically being unknown, hidden or otherwise hard to model. Perhaps this is also why the tidy data paper spends some time talking about notions of functional  dependencies and normal forms, which are also relevant to computations. In fact, tidy data is in Codd's third normal form, which is in turn a more relaxed version of the Boyce-Codd normal form (BCNF). The BCNF is automatically satisfied by operation nodes in a computation frame when viewed as relations.</p> <p>On the one hand, the explicit knowledge of the data generating process makes the job of computation tracking easier in an ontological sense. Wickham remarks that, while easy to disambiguate in concrete examples, the concepts of \"variable\" and \"observation\" are actually hard to define abstractly. Not so for computations: variables are inputs/outputs of functions, and observations are function calls. </p> <p>But on the other hand, this detailed knowledge also gives us more complex situations to handle, such as feedback loops, branching pipelines, and aggregation/decomposition to name a few. This means we need more expressive tools and grammars to handle these situations. Furthermore, even if functions impose a notion of variables and observations, this does not prevent one from designing function interfaces poorly, which can in turn lead to messy computations.</p>"},{"location":"blog/01_cf/#graphs-databases-categories","title":"Graphs, databases, categories","text":"<p>There's a rich history of work in relational databases and graph databases, and CFs share some similarities with both:</p> <ul> <li>A CF can be seen as a relational database, with a table for each operation and each variable. The operation tables have columns labeled by the inputs/outputs of the operation, and the values in these columns are pointers to the corresponding input/output values, which are stored in the (single-column) variable tables.</li> <li>The <code>.df()</code> method works by performing an outer join operation (in a specific order) on the tables containing the full computational history of all final values in the CF.</li> <li>Similarly, a CF can be seen as a graph database, where the nodes are calls and values, and the variables and operations serve as \"node types\".</li> <li>Correspondingly, some operations, such as expansion or finding the dependencies/dependents of some nodes/values can be seen as graph traversal operations.</li> </ul> <p>Finally, the CF data structure can be seen as a kind of \"functor\" from a finite category (roughly speaking the call graph) to the category of sets. Some consequences of this perspective \u2014 which combines graphs and databases \u2014 are presented e.g. in this paper</p>"},{"location":"blog/02_deps/","title":"Practical dependency tracking for Python function calls","text":""},{"location":"blog/02_deps/#tldr","title":"tl;dr","text":"<p>Tracking the code and data accessed by a (Python) function call is a broadly useful primitive, from drawing dependency graphs, to debugging and profiling, to cache invalidation. This post is a journey through the landscape of possible implementations, with a focus on solutions that are transparent, robust and applicable to practical production scenarios. A minimal viable implementation in &lt;100 lines of code is included (gist); a practical implementation is part of mandala, a library for incremental computing and experiment management.</p> <p></p> <p>Figure. Dependencies extracted from a call to the function <code>train_model</code> in module <code>main</code>: functions (blue), methods (purple) and globals (red) </p>"},{"location":"blog/02_deps/#outline","title":"Outline","text":"<ul> <li>motivation:   the use case I ran into, and technical requirements that came out of it</li> <li>proposed solution: a prototype in &lt;100 lines of code you   can customize to your own use cases</li> <li>what doesn't work and why: alternative designs and why   I decided against them</li> </ul>"},{"location":"blog/02_deps/#motivation-efficient-and-reproducible-computational-experiments","title":"Motivation: efficient and reproducible computational experiments","text":"<p>Function dependency information is useful for all sorts of stuff, from drawing pretty call graphs to debugging and profiling to measuring test coverage. Personally, I wanted to cache function calls and detect when a cached call is no longer valid because the code and/or data it depends on have changed. This means that, for each call, you must know the exact functions/methods it called and globals it accessed. </p> <p>Concretely, such a memoization tool can save a lot of computer/programmer time in computational fields like machine learning and data science. Projects there typically have many moving pieces, and each piece can change at any time. It's common for a change in one piece to affect only some steps of a project, and re-running everything from scratch takes too long: you want to do the \"new\" computations only.</p> <p>Manually keeping track of this is error-prone and distracts you from your actual project! There exist tools like dvc that can sort of automate this, but they are generally more rigid - e.g., require you to break your code up into scripts instead of functions. Instead, I wanted something simpler to understand and add to existing code in e.g. your Jupyter notebook, so you can do your work in the most straightforward way with minimal boilerplate.</p>"},{"location":"blog/02_deps/#technical-requirements","title":"Technical requirements","text":"<p>Deploying dependency tracking in a production ML/DS system poses more challenges than using it for e.g. debugging/profiling, because it's now part of all the computations you do! Unfortunately, I'm not aware of a tool that meets all the requirements of this use case:</p> <ul> <li>track the dependencies actually accessed by each call (including global   variable accesses) as opposed to an over- or under-estimate</li> <li>easily limit the tracked dependencies to user code   (library functions typically don't change, even over relatively long projects)</li> <li>report/abort when a dependency cannot be tracked, e.g. when a function accesses a closure or a global variable that can't be hashed</li> <li>be robust and non-invasive to the main computational process so that   your code behaves as it would without tracking</li> <li>introduce low performance overhead, which is particularly important in   fast-feedback interactive settings, like exploratory computations in Jupyter notebooks.</li> <li>...and more (e.g. deal with concurrency/parallelism, which we won't get to here)</li> </ul> <p>As we'll see later, take together, these requirements rule out several standard approaches: static analyzers (which can over- and under-estimate dependencies), Python's <code>sys.settrace</code> (which is too invasive and inefficient), and profilers (which are designed to provide aggregate statistics post-execution).</p>"},{"location":"blog/02_deps/#proposed-solution","title":"Proposed solution","text":"<p>After tinkering with various magical ways to gather this data using Python internals, I found out that none of them really fit all the requirements of my use case. What ended up working was something simple but ultimately more reliable and efficient:</p> <ul> <li>decorate all the functions whose code you want to track. The decorator implements its own call stack, separate from Python's, that tracks just these functions' calls.</li> <li>the decorator also hooks into the <code>__globals__</code> of the function object (the dictionary of globals available to the function), and tracks every access to it. I learned this from this blog post.</li> </ul> <p>The only downside is that you have to explicitly decorate the functions/classes you want to track (you could do this automatically with an import hook, but that's perhaps too much magic). The full code + an example is in this gist.</p>"},{"location":"blog/02_deps/#the-decorator","title":"The decorator","text":"<p>The <code>@track</code> decorator simply modifies a function <code>f</code> to emit an event to the global <code>Tracer</code> object (defined below) right before and after it is called: <pre><code>from types import FunctionType\nfrom functools import wraps\nfrom typing import Optional\n\nclass TracerState:\n    current: Optional['Tracer'] = None\n\ndef track(f: FunctionType):\n\n    @wraps(f) # to make the wrapped function look like `f`\n    def wrapper(*args, **kwargs):\n        tracer = TracerState.current\n        if tracer is not None:\n            tracer.register_call(func=f) # put call to `f` on stack\n            result = f(*args, **kwargs)\n            tracer.register_return() # pop call to `f` from stack\n            return result\n        else:\n            return f(*args, **kwargs)\n\n    return wrapper\n</code></pre></p>"},{"location":"blog/02_deps/#the-tracer","title":"The tracer","text":"<p>Most importantly, the tracer keeps track of calls to decorated functions by putting a call on the stack right before a decorated function is called, and popping the top call when a decorated function returns. Using the call stack, you can derive all sorts of other useful information. For example, the implementation below uses the stack to build a dynamic call graph (represented as a list of edges for simplicity). It's implemented as a context manager that only tracks calls that happen inside a <code>with</code> block: <pre><code>from typing import Callable\n\nclass Tracer:\n    def __init__(self):\n        # call stack of (module name, qualified function/method name) tuples\n        self.stack = [] \n        # list of (caller module, caller qualname, callee module, callee\n        # qualname) tuples\n        self.graph = [] \n\n    def register_call(self, func: Callable): \n        # Add a call to the stack and the graph\n        module_name, qual_name = func.__module__, func.__qualname__\n        self.stack.append((module_name, qual_name))\n        if len(self.stack) &gt; 1:\n            caller_module, caller_qual_name = self.stack[-2]\n            self.graph.append((caller_module, caller_qual_name,\n                               module_name, qual_name))\n\n    def register_return(self):\n        self.stack.pop()\n\n    def __enter__(self):\n        TracerState.current = self\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        TracerState.current = None\n</code></pre> Note that we use the qualified name of a function, which contains all the nested class names in the case of methods.</p>"},{"location":"blog/02_deps/#a-minimal-example","title":"A minimal example","text":"<p>You can already use this as follows: <pre><code>In [1]: @track\n   ...: def f(x):\n   ...:     return x + 1\n   ...: \n\nIn [2]: @track\n   ...: def g(x):\n   ...:     return f(x) + 1\n   ...: \n\nIn [3]: with Tracer() as t:\n   ...:     g(23)\n   ...: \n\nIn [4]: t.graph\nOut[4]: [('__main__', 'g', '__main__', 'f')]\n</code></pre></p>"},{"location":"blog/02_deps/#adding-globals-tracking","title":"Adding globals tracking","text":"<p>When a function <code>f</code> is called, how does Python know how names in the code of <code>f</code> correspond to values in the program? You can read about this at length in Python's documentation, but the gist of it is that the relevant scopes are looked up in the following order:</p> <ul> <li>local: <code>func</code>'s own scope,</li> <li>enclosing: any scopes of functions inside which <code>func</code> is defined, accessible through <code>func.__closure__</code></li> <li>global: the the namespace of the function's module, accessible through <code>func.__globals__</code>.</li> <li>builtin: Python's imported-by-default objects</li> </ul> <p>It even has a \"catchy\" acronym: the LEGB rule. For now, we'll assume there's no enclosing scope. In this case, we are really only interested in accesses to <code>__globals__</code>. As it turns out, we can substitute a function's <code>__globals__</code> - which is a dictionary - with a modified object that behaves exactly the same but also tracks accesses. For this, we add a <code>register_global_access</code> method to <code>Tracer</code> (which adds globals to the graph as key-value pairs to disambiguate them from function calls), and define a simple subclass of <code>dict</code>: <pre><code>class Tracer:\n    ...\n\n    def register_global_access(self, key: str, value): # &lt;- ADD THIS METHOD\n        assert len(self.stack) &gt; 0\n        caller_module, caller_qual_name = self.stack[-1]\n        self.graph.append((caller_module, caller_qual_name, {key: value}))\n\n    ...\n\nfrom typing import Any\n\nclass TrackedDict(dict):\n    def __init__(self, original: dict):\n        self.__original__ = original\n\n    def __getitem__(self, __key: str) -&gt; Any:\n        value = self.__original__.__getitem__(__key)\n        if TracerState.current is not None:\n            tracer = TracerState.current\n            tracer.register_global_access(key=__key, value=value)\n        return value\n</code></pre> Implementing the strategy is somewhat complicated by the fact that <code>__globals__</code> is a read-only attribute and can't be updated in-place. The below helper copies a function, keeping everything the same except for using a <code>TrackedDict</code> for the globals: <pre><code>import copy\nfrom functools import update_wrapper\n\ndef make_tracked_copy(f: FunctionType) -&gt; FunctionType:\n    result = FunctionType(\n        code=f.__code__,\n        globals=TrackedDict(f.__globals__),\n        name=f.__name__,\n        argdefs=f.__defaults__,\n        closure=f.__closure__,\n    )\n    result = update_wrapper(result, f)\n    result.__module__ = f.__module__\n    result.__kwdefaults__ = copy.deepcopy(f.__kwdefaults__)\n    result.__annotations__ = copy.deepcopy(f.__annotations__)\n    return result\n</code></pre> Note that, even though we use <code>update_wrapper</code>, some properties of <code>f</code> must be carried over manually to <code>f</code>'s copy; maybe there are some others you need to copy as well depending on your use case. You can now modify the <code>track</code> decorator as <pre><code>def track(f: FunctionType):\n    f = make_tracked_copy(f) # add this line\n\n    @wraps(f)\n    ...\n</code></pre></p>"},{"location":"blog/02_deps/#a-more-interesting-example","title":"A more interesting example","text":"<p>Here's a more interesting example of all the stuff we covered so far in action: tracking global variables, functions, and even nested class methods: <pre><code>A = 23\nB = 42\n\n@track\ndef f(x):\n    return x + A\n\nclass C:\n    @track\n    def __init__(self, x):\n        self.x = x + B\n\n    @track\n    def m(self, y):\n        return self.x + y\n\n    class D:\n        @track\n        def __init__(self, x):\n            self.x = x + f(x)\n\n        @track\n        def m(self, y):\n            return y + A\n\n@track\ndef g(x):\n    if x % 2 == 0:\n        return C(x).m(x)\n    else:\n        return C.D(x).m(x)\n</code></pre> As expected, you get different results for the two branches of <code>g</code>: <pre><code>In [1]: with Tracer() as t:\n   ...:     g(23)\n   ...: \n\nIn [2]: t.graph\nOut[2]: \n[('__main__', 'g', {'C': __main__.C}),\n ('__main__', 'g', '__main__', 'C.D.__init__'),\n ('__main__', 'C.D.__init__', {'f': &lt;function __main__.f(x)&gt;}),\n ('__main__', 'C.D.__init__', '__main__', 'f'),\n ('__main__', 'f', {'A': 23}),\n ('__main__', 'g', '__main__', 'C.D.m'),\n ('__main__', 'C.D.m', {'A': 23})]\n\nIn [3]: with Tracer() as t:\n    ...:     g(42)\n    ...: \n\nIn [4]: t.graph\nOut[4]: \n[('__main__', 'g', {'C': __main__.C}),\n ('__main__', 'g', '__main__', 'C.__init__'),\n ('__main__', 'C.__init__', {'B': 42}),\n ('__main__', 'g', '__main__', 'C.m')]\n</code></pre></p>"},{"location":"blog/02_deps/#beyond-the-prototype","title":"Beyond the prototype","text":"<p>The code so far already has all the key components of a solution. Even better, it's easily customizable: it's up to you to decide whether some calls or globals should be excluded, how to respond to changes in dependencies, etc. To make this scaffolding more robust and practical, you might want to add a few minor improvements. I found the following helpful:</p> <ul> <li>replace global variable values with content hashes, because otherwise you   might end up tracking a lot of state that is not garbage-collected</li> <li>apply the decorator to entire classes by decorating each of their methods   automatically. This saves you at least some of the manual work!</li> <li>filter out function/method/class accesses when tracking globals accesses.   As you can see above, <code>C.D.__init__</code> accesses the global variable <code>f</code>, but you   probably don't care about this most of the time.</li> <li>check for closures using the <code>__closure__</code> attribute of the function being called. Closures are more complex to track than code available at import time. To make life simpler, you may choose to detect closures at runtime and raise an error to disable them.</li> <li>make the decorator work well with other decorators: Python decorators are great, but also a total anarchy. Anybody can use something like <code>lambda x: None</code> as a decorator! If it's in your power, you should put <code>@track</code> on the bottom of decorator stacks (i.e. directly over the function definition). Otherwise, cross your fingers that whoever implemented the decorators in your code was nice and exposed a <code>__wrapped__</code> attribute. Take a look at the Python docs. </li> <li>use import hooks to automatically decorate your code at import time, if you dare.</li> </ul>"},{"location":"blog/02_deps/#what-doesnt-work-and-why","title":"What doesn't work, and why","text":"<p>What follows is a tour through some Python tools/internals that can address parts of the problem, but ultimately fail to satisfy all requirements:</p> <ul> <li><code>sys.settrace</code> is a solid alternative, but introduces too much unavoidable overhead in practical interactive scenarios, and can't track dynamic accesses to the globals.</li> <li>profilers like cProfile introduce less overhead than <code>sys.settrace</code>. However,   they don't track per-call dependencies, don't give you runtime control over what   the program does (so you can't e.g. react to a dependency that you fundamentally   can't track), and make it harder to extract full dependency information.</li> <li>static analysis can discover more/fewer dependencies than   the ground truth, and is altogether messier to implement.</li> </ul>"},{"location":"blog/02_deps/#syssettrace","title":"<code>sys.settrace</code>","text":"<p>Python is a famously (notoriously?) hackable language: it lets you hook into a lot of the internal machinery of the interpreter itself. One such piece of magic is <code>sys.settrace</code>, which allows you to install a hook that gets called for each of the main events of the interpreter: function calls/returns, and even executing a single line of code in a function (for example, this is how coverage can be so fine-grained).</p> <p>Using <code>sys.settrace</code>, we can obtain something very similar to the solution developed above, but without the need to explicitly decorate your code. Here is a minimal example of a stateful context manager using <code>settrace</code> to maintain a call stack of the functions that get called and the modules they originate from: <pre><code>import sys, types \n\nclass Tracer:\n    def __init__(self):\n        # stack of (module name, function name) tuples\n        self.call_stack = []\n\n    def __enter__(self):\n        def tracer(frame: types.FrameType, event: str, arg):\n            # the name of the function being executed\n            func_name = frame.f_code.co_name\n            # the name of the module in which the function is defined\n            module_name = frame.f_globals.get(\"__name__\")\n            if event == 'call': # function call\n                self.call_stack.append((module_name, func_name))\n                print(f\"Calling {module_name}.{func_name}\")\n            elif event == 'return': # function return\n                ret_module, ret_func = self.call_stack.pop()\n                print(f\"Returning from {ret_module}.{ret_func}\")\n            else:\n                pass\n            return tracer\n\n        sys.settrace(tracer) # enable tracing\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        sys.settrace(None) # disable tracing\n</code></pre> The <code>frame</code> object is what Python puts on its call stack, and contains data about the function being called, its bytecode, who called it, etc. You can use this context manager as follows: <pre><code>### in funcs.py\ndef f(x):\n    return x + 1\n\n### in IPython session\nIn [1]: from funcs import *\n\nIn [2]: def g(x):\n   ...:     return f(x) + 1\n   ...: \n\nIn [3]: with Tracer():\n   ...:     g(23)\n   ...: \nCalling __main__.g\nCalling funcs.f\nReturning from funcs.f\nReturning from __main__.g\nCalling funcs.__exit__ # you'd have to manually remove this one\n</code></pre> This can be extended with more features much like the decorator-based tracer. There are some <code>settrace</code>-specific problems you have to deal with though:</p> <ul> <li>limit the dependencies to user code by looking at the module in which the function is defined, getting its path, and deciding if it's a user's file or not.</li> <li>get the qualified name: this is frustratingly not readily available as   part of the <code>frame</code> object. You need some hacks to extract it: <pre><code>def get_qualname_from_frame(frame: types.FrameType) -&gt; str:\n    arg_names = frame.f_code.co_varnames[: frame.f_code.co_argcount]\n    if len(arg_names) &gt; 0 and arg_names[0] == 'self':\n        cls_candidate = frame.f_locals['self'].__class__\n        method_candidate = cls_candidate.__dict__.get(frame.f_code.co_name)\n        if method_candidate is not None and method_candidate.__code__ is frame.f_code:\n            return method_candidate.__qualname__\n    return frame.f_code.co_name\n</code></pre></li> <li>skip over non-function frames: the interpreter assigns comprehensions,   generators and <code>lambda</code>-calls their own frames. You have to check for this   using <code>frame.f_code.co_name</code>, and assign their dependencies to the closest \"actual\"   function call on the stack.</li> </ul>"},{"location":"blog/02_deps/#syssettraces-unavoidable-overhead","title":"<code>sys.settrace</code>'s unavoidable overhead","text":"<p>A good reason to avoid <code>settrace</code> in production code is that it's too magical for its own good. However, the real deal-breaker for my use case was the impossible-to-avoid factor by which it slows down some kinds of code.</p> <p>The crux is that the trace function is inherently called for each <code>call</code> event, including calls to library functions that you don't care about tracking, because they typically don't change over the course of a months-long project. For relatively fast function calls (on the order of seconds), you may get an order-of-magnitude slowdown if the call involves many sub-calls. This is unacceptable for interactive workflows!</p> <p>You might think you could fix that with a bit of manual work by excluding such library code from the tracing. Indeed, you can define a simple context manager that temporarily suspends the current trace: <pre><code>class Suspend:\n    def __init__(self):\n        self.suspended_trace = None\n\n    def __enter__(self) -&gt; \"Suspend\":\n        if sys.gettrace() is not None:\n            self.suspended_trace = sys.gettrace()\n            sys.settrace(None)\n        return self\n\n    def __exit__(self, *exc_info):\n        if self.suspended_trace is not None:\n            sys.settrace(self.suspended_trace)\n            self.suspended_trace = None\n</code></pre> Then you can use it like this: <pre><code>def my_tracked_func(...):\n    ...\n    a = another_tracked_func()\n    with Suspend():\n        b = some_library_calls_you_dont_want_to_track(a)\n        ...\n    ...\n</code></pre> However, there are cases when you simply can't do that!  To give a concrete example, I was going though the code for this blog post, and I ran into an interesting scenario. A user-defined function <code>f</code> was passed into <code>jax.lax.scan</code>, as a way to speed up certain applications of <code>f</code>: <pre><code>def run_fsm(fsm: FSM, inputs):\n  def f(s, x):\n    y  = jp.einsum('x,s,xsy-&gt;y', x, s, fsm.R)\n    s1 = jp.einsum('x,s,xst-&gt;t', x, s, fsm.T)\n    return s1, (y, s1)\n  _, (outputs, states) = jax.lax.scan(f, fsm.s0, inputs) # THIS IS BAD\n  return outputs, jp.vstack([fsm.s0, states]\n</code></pre> Because you're passing your function to the library and it can call it however it likes, you lose the ability to separate the executions of your code from those of library code. The <code>Suspend</code> trick can't work: you're forced to trace all the internal calls the library makes alongside the calls to your code.</p>"},{"location":"blog/02_deps/#cprofile","title":"<code>cProfile</code>","text":"<p>A profiler is a dynamic program analysis tool typically used to pinpoint performance bottlenecks in code. There are two main kinds: </p> <ul> <li>statistical profilers sample a program's state (e.g. call stack, memory allocation) at regular intervals. This reduces overhead, while still detecting functions where the program spends a lot of time.</li> <li>deterministic   profilers   by contrast record every function call that happens in the program, and   accordingly suffer higher overhead.</li> </ul> <p>In dependency tracking, failing to notice even a single dependency that is fast and gets called rarely can have disastrous results, so statistical profilers are not really an option: you need a deterministic one. Since Python's interpreter adds so much overhead anyway, Python's built-in (deterministic) profilers don't introduce that much over-overhead. Of the two, <code>cProfile</code> is faster. Here's the minimal implementation of a tracer based on profiling: <pre><code>import cProfile\nimport pstats\n\nclass ProfilingTracer:\n    def __init__(self):\n        self._profiler = cProfile.Profile()\n\n    def __enter__(self):\n        self._profiler.enable()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._profiler.disable()\n        stats = pstats.Stats(self._profiler)\n        stats.print_callees()\n</code></pre> Conveniently, the output of the profiler has a method <code>print_callees</code> that prints all the functions that were called by each given function in the profiled block of code. We can run it on the code from before to get this (simplified for readability) output: <pre><code>Function                                  called...\n                ncalls  tottime  cumtime\n(__init__)  -&gt;       1    0.000    0.000  (f)\n(m)         -&gt; \n(g)         -&gt;       1    0.000    0.000  (__init__)\n                     1    0.000    0.000  (m)\n                     1    0.000    0.000  (__init__)\n                     1    0.000    0.000  (m)\n(m)         -&gt; \n(f)         -&gt; \n(__init__)  -&gt; \n</code></pre> The downsides of this approach become clear:</p> <ul> <li>data is aggregated across all calls to a given function: in the tracked   code, we call <code>g</code> twice, and the two calls have different dependencies. But in   the final report of the profiler, the two calls are grouped together. So we   have no way of tracking per-call dependencies based on this data!</li> <li>no qualified names and globals: you would have to do some extra work (e.g. looking at   line numbers, which do appear in the profiler report) to disambiguate the   classes methods come from. And as with <code>settrace</code>, you have no way to detect   globals accesses.</li> <li>no runtime control over dependencies: the profiler report is an   after-the-fact summary of what happened; you don't have the option to abort if   you detect a bad dependency.</li> </ul> <p>Of course, it's not surprising that profilers have a hard time tracking fine-grained dependencies: they weren't designed for that!</p>"},{"location":"blog/02_deps/#static-analysis","title":"Static analysis","text":"<p>Finally, static analysis is a collection of methods for deducing program properties from source code alone, i.e. without running the program. For example, code2flow is a static call graph builder for Python. In Python, static analyses typically proceed from the abstract syntax tree and/or the bytecode.</p> <p>This approach doesn't interact with your running program at all, which is great for performance and generally letting your code work the way it's supposed to. Unfortunately, it's fundamentally flawed for many other reasons:</p> <ul> <li>false positives: suppose <code>f</code>'s source code contains calls to <code>g</code> and <code>h</code>, but some calls to <code>f</code> use only <code>g</code>, and others only <code>h</code>.  A static analysis would miss that and declare <code>g</code> and <code>h</code> as dependencies of all calls.</li> <li>false negatives: your function can call another function in all sorts of   weird ways that cannot be inferred from the syntax tree and/or bytecode alone. For an extreme   example, consider something like <code>a = eval('my_' + 'function(42)')</code>. Because   of variations of the halting problem, it's both difficult   and impossible for a static analysis to   determine what a function will do at runtime.</li> <li>high implementation complexity: even if you adopt a best-effort approach,   you have to do a lot of work to figure out the precise function called by   an expression like <code>a().b().c()</code>.</li> </ul> <p>Overall, using static analysis is not worth the hassle given the ultimately coarse-grained information it can provide.</p>"},{"location":"topics/01_storage_and_ops/","title":"<code>Storage</code> &amp; the <code>@op</code> Decorator","text":"<p>A <code>Storage</code> object holds all data (saved calls, code and dependencies) for a collection of memoized functions. In a given project, you should have just one <code>Storage</code> and many <code>@op</code>s connected to it. This way, the calls to memoized functions create a queriable web of interlinked objects.  </p> <pre><code># for Google Colab\ntry:\n    import google.colab\n    !pip install git+https://github.com/amakelov/mandala\nexcept:\n    pass\n</code></pre>"},{"location":"topics/01_storage_and_ops/#creating-a-storage","title":"Creating a <code>Storage</code>","text":"<p>When creating a storage, you must decide if it will be in-memory or persisted on disk, and whether the storage will automatically version the <code>@op</code>s used with it:</p> <pre><code>from mandala.imports import Storage\nimport os\n\nDB_PATH = 'my_persistent_storage.db'\nif os.path.exists(DB_PATH):\n    os.remove(DB_PATH)\n\nstorage = Storage(\n    # omit for an in-memory storage\n    db_path=DB_PATH,\n    # omit to disable automatic dependency tracking &amp; versioning\n    # use \"__main__\" to only track functions defined in the current session\n    deps_path='__main__', \n)\n</code></pre>"},{"location":"topics/01_storage_and_ops/#creating-ops-and-saving-calls-to-them","title":"Creating <code>@op</code>s and saving calls to them","text":"<p>Any Python function can be decorated with <code>@op</code>:</p> <pre><code>from mandala.imports import op\n\n@op \ndef sum_args(a, *args, b=1, **kwargs):\n    return a + sum(args) + b + sum(kwargs.values())\n</code></pre> <p>In general, calling <code>sum_args</code> will behave as if the <code>@op</code> decorator is not there. <code>@op</code>-decorated functions will interact with a <code>Storage</code> instance only when called inside a <code>with storage:</code> block:</p> <pre><code>with storage: # all `@op` calls inside this block use `storage`\n    s = sum_args(6, 7, 8, 9, c=11,)\n    print(s)\n</code></pre> <pre><code>AtomRef(42, hid=168...)\n</code></pre> <p>This code runs the call to <code>sum_args</code>, and saves the inputs and outputs in the <code>storage</code> object, so that doing the same call later will directly load the saved outputs.</p>"},{"location":"topics/01_storage_and_ops/#when-should-something-be-an-op","title":"When should something be an <code>@op</code>?","text":"<p>As a general guide, you should make something an <code>@op</code> if you want to save its outputs, e.g. if they take a long time to compute but you need them for later analysis. Since <code>@op</code> encourages composition, you should aim to have <code>@op</code>s work on the outputs of other <code>@op</code>s, or on the collections and/or items of outputs of other <code>@op</code>s.</p>"},{"location":"topics/01_storage_and_ops/#working-with-op-outputs-refs","title":"Working with <code>@op</code> outputs (<code>Ref</code>s)","text":"<p>The objects (e.g. <code>s</code>) returned by <code>@op</code>s are always instances of a subclass of <code>Ref</code> (e.g., <code>AtomRef</code>), i.e.  references to objects in the storage. Every <code>Ref</code> contains two metadata fields:</p> <ul> <li><code>cid</code>: a hash of the content of the object</li> <li><code>hid</code>: a hash of the computational history of the object, which is the precise composition of <code>@op</code>s that created this ref.  </li> </ul> <p>Two <code>Ref</code>s with the same <code>cid</code> may have different <code>hid</code>s, and <code>hid</code> is the unique identifier of <code>Ref</code>s in the storage. However, only 1 copy per unique <code>cid</code> is stored to avoid duplication in the storage.</p>"},{"location":"topics/01_storage_and_ops/#refs-can-be-in-memory-or-not","title":"<code>Ref</code>s can be in memory or not","text":"<p>Additionally, <code>Ref</code>s have the <code>in_memory</code> property, which indicates if the underlying object is present in the <code>Ref</code> or if this is a \"lazy\" <code>Ref</code> which only contains metadata. <code>Ref</code>s are only loaded in memory when needed for a new call to an <code>@op</code>. For example, re-running the last code block:</p> <pre><code>with storage: \n    s = sum_args(6, 7, 8, 9, c=11,)\n    print(s)\n</code></pre> <pre><code>AtomRef(hid=168..., in_memory=False)\n</code></pre> <p>To get the object wrapped by a <code>Ref</code>, call <code>storage.unwrap</code>:</p> <pre><code>storage.unwrap(s) # loads from storage only if necessary\n</code></pre> <pre><code>42\n</code></pre>"},{"location":"topics/01_storage_and_ops/#other-useful-storage-methods","title":"Other useful <code>Storage</code> methods","text":"<ul> <li><code>Storage.attach(inplace: bool)</code>: like <code>unwrap</code>, but puts the objects in the <code>Ref</code>s if they are not in-memory.</li> <li><code>Storage.load_ref(hid: str, in_memory: bool)</code>: load a <code>Ref</code> by its history ID, optionally also loading the underlying object.</li> </ul> <pre><code>print(storage.attach(obj=s, inplace=False))\nprint(storage.load_ref(s.hid))\n</code></pre> <pre><code>AtomRef(42, hid=168...)\nAtomRef(42, hid=168...)\n</code></pre>"},{"location":"topics/01_storage_and_ops/#working-with-call-objects","title":"Working with <code>Call</code> objects","text":"<p>Besides <code>Ref</code>s, the other kind of object in the storage is the <code>Call</code>, which stores references to the inputs and outputs of a call to an <code>@op</code>, together with metadata that mirrors the <code>Ref</code> metadata:</p> <ul> <li><code>Call.cid</code>: a content ID for the call, based on the <code>@op</code>'s identity, its version at the time of the call, and the <code>cid</code>s of the inputs</li> <li><code>Call.hid</code>: a history ID for the call, the same as <code>Call.cid</code>, but using the  <code>hid</code>s of the inputs.</li> </ul> <p>For every <code>Ref</code> history ID, there's at most one <code>Call</code> that has an output with this history ID, and if it exists, this call can be found by calling <code>storage.get_ref_creator()</code>: </p> <pre><code>call = storage.get_ref_creator(ref=s)\nprint(call)\ndisplay(call.inputs)\ndisplay(call.outputs)\n</code></pre> <pre><code>Call(sum_args, hid=f99...)\n\n\n\n{'a': AtomRef(hid=c6a..., in_memory=False),\n 'args_0': AtomRef(hid=e0f..., in_memory=False),\n 'args_1': AtomRef(hid=479..., in_memory=False),\n 'args_2': AtomRef(hid=c37..., in_memory=False),\n 'b': AtomRef(hid=610..., in_memory=False),\n 'c': AtomRef(hid=a33..., in_memory=False)}\n\n\n\n{'output_0': AtomRef(hid=168..., in_memory=False)}\n</code></pre>"},{"location":"topics/02_retracing/","title":"Patterns for Incremental Computation &amp; Development","text":"<p><code>@op</code>-decorated functions are designed to be composed with one another. This enables the same piece of imperative code to adapt to multiple goals depending on the situation: </p> <ul> <li>saving new <code>@op</code> calls and/or loading previous ones;</li> <li>cheaply resuming an <code>@op</code> program after a failure;</li> <li>incrementally adding more logic and computations to the same code without re-doing work.</li> </ul> <p>This section of the documentation does not introduce new methods or classes. Instead, it demonstrates the programming patterns needed to make effective use of <code>mandala</code>'s memoization capabilities.</p>"},{"location":"topics/02_retracing/#how-op-encourages-composition","title":"How <code>@op</code> encourages composition","text":"<p>There are several ways in which the <code>@op</code> decorator encourages (and even enforces) composition of <code>@op</code>s:</p> <ul> <li><code>@op</code>s return special objects, <code>Ref</code>s, which prevents accidentally calling  a non-<code>@op</code> on the output of an <code>@op</code></li> <li>If the inputs to an <code>@op</code> call are already <code>Ref</code>s, this speeds up the cache lookups.</li> <li>If the call can be reused, the input <code>Ref</code>s don't even need to be in memory (because the lookup is based only on <code>Ref</code> metadata).</li> <li>When <code>@op</code>s are composed, computational history propagates through this composition. This is automatically leveraged by <code>ComputationFrame</code>s when querying the storage.</li> <li>Though not documented here, <code>@op</code>s can natively handle Python collections like lists and dicts. This </li> </ul> <p>When <code>@op</code>s are composed in this way, the entire computation becomes \"end-to-end memoized\". </p>"},{"location":"topics/02_retracing/#toy-ml-pipeline-example","title":"Toy ML pipeline example","text":"<p>Here's a small example of a machine learning pipeline:</p> <pre><code># for Google Colab\ntry:\n    import google.colab\n    !pip install git+https://github.com/amakelov/mandala\nexcept:\n    pass\n</code></pre> <pre><code>from mandala.imports import *\nfrom sklearn.datasets import load_digits\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n@op\ndef load_data(n_class=2):\n    print(\"Loading data\")\n    return load_digits(n_class=n_class, return_X_y=True)\n\n@op\ndef train_model(X, y, n_estimators=5):\n    print(\"Training model\")\n    return RandomForestClassifier(n_estimators=n_estimators,\n                                  max_depth=2).fit(X, y)\n\n@op\ndef get_acc(model, X, y):\n    print(\"Getting accuracy\")\n    return round(accuracy_score(y_pred=model.predict(X), y_true=y), 2)\n\nstorage = Storage()\n\nwith storage:\n    X, y = load_data() \n    model = train_model(X, y)\n    acc = get_acc(model, X, y)\n    print(acc)\n</code></pre> <pre><code>Loading data\nTraining model\nGetting accuracy\nAtomRef(1.0, hid=d16...)\n</code></pre>"},{"location":"topics/02_retracing/#retracing-your-steps-with-memoization","title":"Retracing your steps with memoization","text":"<p>Running the computation again will not execute any calls, because it will exactly retrace calls that happened in the past. Moreover, the retracing is lazy: none of the values along the way are actually loaded from storage:</p> <pre><code>with storage:\n    X, y = load_data() \n    print(X, y)\n    model = train_model(X, y)\n    print(model)\n    acc = get_acc(model, X, y)\n    print(acc)\n</code></pre> <pre><code>AtomRef(hid=d0f..., in_memory=False) AtomRef(hid=f1a..., in_memory=False)\nAtomRef(hid=caf..., in_memory=False)\nAtomRef(hid=d16..., in_memory=False)\n</code></pre> <p>This puts all the <code>Ref</code>s along the way in your local variables (as if you've just ran the computation), which lets you easily inspect any intermediate variables in this <code>@op</code> composition:</p> <pre><code>storage.unwrap(acc)\n</code></pre> <pre><code>1.0\n</code></pre>"},{"location":"topics/02_retracing/#adding-new-calls-in-place-in-op-based-programs","title":"Adding new calls \"in-place\" in <code>@op</code>-based programs","text":"<p>With <code>mandala</code>, you don't need to think about what's already been computed and split up code based on that. All past results are automatically reused, so you can directly build upon the existing composition of <code>@op</code>s when you want to add new functions and/or run old ones with different parameters:</p> <pre><code># reuse the previous code to loop over more values of n_class and n_estimators \nwith storage:\n    for n_class in (2, 5,):\n        X, y = load_data(n_class) \n        for n_estimators in (5, 10):\n            model = train_model(X, y, n_estimators=n_estimators)\n            acc = get_acc(model, X, y)\n            print(acc)\n</code></pre> <pre><code>AtomRef(hid=d16..., in_memory=False)\nTraining model\nGetting accuracy\nAtomRef(1.0, hid=6fd...)\nLoading data\nTraining model\nGetting accuracy\nAtomRef(0.88, hid=158...)\nTraining model\nGetting accuracy\nAtomRef(0.88, hid=214...)\n</code></pre> <p>Note that the first value of <code>acc</code> from the nested loop is with <code>in_memory=False</code>, because it was reused from the call we did before; the other values are in memory, as they were freshly computed. </p> <p>This pattern lets you incrementally build towards the final computations you want without worrying about how results will be reused.</p>"},{"location":"topics/02_retracing/#using-control-flow-efficiently-with-ops","title":"Using control flow efficiently with <code>@op</code>s","text":"<p>Because the unit of storage is the function call (as opposed to an entire script or notebook), you can transparently use Pythonic control flow. If the control flow depends on a <code>Ref</code>, you can explicitly load just this <code>Ref</code> in memory using <code>storage.unwrap</code>:</p> <pre><code>with storage:\n    for n_class in (2, 5,):\n        X, y = load_data(n_class) \n        for n_estimators in (5, 10):\n            model = train_model(X, y, n_estimators=n_estimators)\n            acc = get_acc(model, X, y)\n            if storage.unwrap(acc) &gt; 0.9: # load only the `Ref`s needed for control flow\n                print(n_class, n_estimators, storage.unwrap(acc))\n</code></pre> <pre><code>2 5 1.0\n2 10 1.0\n</code></pre>"},{"location":"topics/02_retracing/#memoized-code-as-storage-interface","title":"Memoized code as storage interface","text":"<p>An end-to-end memoized composition of <code>@op</code>s is like an \"imperative\" storage interface. You can modify the code to only focus on particular results of interest:</p> <pre><code>with storage:\n    for n_class in (5,):\n        X, y = load_data(n_class) \n        for n_estimators in (5,):\n            model = train_model(X, y, n_estimators=n_estimators)\n            acc = get_acc(model, X, y)\n            print(storage.unwrap(acc), storage.unwrap(model))\n</code></pre> <pre><code>0.88 RandomForestClassifier(max_depth=2, n_estimators=5)\n</code></pre>"},{"location":"topics/03_cf/","title":"Query the Storage with <code>ComputationFrame</code>s","text":""},{"location":"topics/03_cf/#why-computationframes","title":"Why <code>ComputationFrame</code>s?","text":"<p>The <code>ComputationFrame</code> data structure formalizes the natural/intuitive way you think of the \"web\" of saved <code>@op</code> calls. It gives you a \"grammar\" in which operations over persisted computation graphs that are easy to think of are also easy to implement.</p> <p>In computational projects, all queries boil down to how some variables depend on other variables: e.g., in ML you often care about what input parameters lead to final results with certain metrics. The <code>mandala</code> storage can automatically answer such questions when all operations along the way were <code>@op</code>s, because it represents the \"web\" of saved <code>@op</code> calls, linked by how the outputs of one <code>@op</code> are used as inputs to other <code>@op</code>s.</p> <p>The <code>ComputationFrame</code> (CF) is the data structure used to explore and query this web of calls. It's a high-level view of a collection of <code>@op</code> calls, so that calls that serve the same role are grouped together. The groups of calls form a computational graph of variables and functions, which enables effective &amp; natural high-level operations over storage. </p> <p>This section covers basic tools to get up to speed with CFs. For more advanced usage, see Advanced <code>ComputationFrame</code> tools </p>"},{"location":"topics/03_cf/#typical-workflow","title":"Typical workflow","text":"<p>Using CFs typically goes through the following stages:</p> <ul> <li>creation: initialize a CF in various ways, e.g. from some <code>Ref</code>s, from all calls to an <code>@op</code>, .... This CF will have a limited view of storage because it will involve few (0 or 1) <code>@op</code>s</li> <li>expansion: add more context to the CF by adding new function nodes containing the calls that produced/used some variable(s). The goal of this stage is to incorporate in the CF all variables whose relationships you're interested in.</li> <li>combination &amp; restriction: merge multiple CFs, restrict to subgraphs or  specific values of the variables along some predicates. This lets you focus on the computations you want before making expensive calls to the storage.</li> <li>conversion to a <code>pandas.DataFrame</code>: finally, extract a table representing the relationships between the variables in the CF for downstream analysis.</li> </ul>"},{"location":"topics/03_cf/#toy-ml-pipeline","title":"Toy ML pipeline","text":"<p>In this section, we'll work with the following toy experiment on a small ML pipeline:</p> <pre><code># for Google Colab\ntry:\n    import google.colab\n    !pip install git+https://github.com/amakelov/mandala\nexcept:\n    pass\n</code></pre> <pre><code>### define ops to train &amp; eval a random forest model on digits dataset\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom mandala.imports import *\ntry:\n    import rich\n    from rich import print as pprint\nexcept ImportError:\n    print(\"rich not installed, using default print\")\n    pprint = print\n\nstorage = Storage()\n\nnp.random.seed(0)\n\n@op \ndef generate_dataset(random_seed=42):\n    X, y = load_digits(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_seed)\n    return X_train, X_test, y_train, y_test\n\n@op\ndef train_model(X_train, y_train, n_estimators):\n    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=2)\n    model.fit(X_train, y_train)\n    return model, round(model.score(X_train, y_train), 2)\n\n@op\ndef eval_model(model, X_test, y_test):\n    return round(model.score(X_test, y_test), 2)\n</code></pre> <pre><code>with storage: \n    X_train, X_test, y_train, y_test = generate_dataset()\n    for n_estimators in [10, 20, 40, 80]:\n        model, train_acc = train_model(X_train, y_train, n_estimators=n_estimators)\n        if storage.unwrap(train_acc) &gt; 0.8: # conditional execution\n            test_acc = eval_model(model, X_test, y_test)\n</code></pre>"},{"location":"topics/03_cf/#creating-computationframes","title":"Creating <code>ComputationFrame</code>s","text":"<p>There are several ways to create a CF, all dispatched through the <code>storage.cf()</code> method.</p>"},{"location":"topics/03_cf/#from-a-single-ref","title":"From a single <code>Ref</code>","text":"<p>The simplest example of a CF is to create one from a <code>Ref</code>:</p> <pre><code>cf = storage.cf(test_acc)\npprint(cf) # text description of the CF \ncf.draw(verbose=True) # pictorial representation of the CF\n</code></pre> <pre>ComputationFrame with:\n    1 variable(s) (1 unique refs)\n    0 operation(s) (0 unique calls)\nComputational graph:\n    v\n</pre> <p></p> <p>As the description says, this is a CF with 1 variable (called <code>v</code>) and 0 operations. The variable contains 1 <code>Ref</code>, i.e. 1 value. We can examine the refs in the variables and the calls in the operations:</p> <pre><code>pprint(f'Refs by variable:\\n{cf.refs_by_var()}')\npprint(f'Calls by operation:\\n{cf.calls_by_func()}')\n</code></pre> <pre>Refs by variable:\n{'v': {AtomRef(0.82, hid=11a...)}}\n</pre> <pre>Calls by operation:\n{}\n</pre> <p>To make this more interesting, we can call <code>expand_back()</code> (explained in more detail later) on the CF to add the full computational history of all values in all variables:</p> <pre><code>cf.expand_back(inplace=True, recursive=True)\npprint(cf)\ncf.draw(verbose=True)\n</code></pre> <pre>ComputationFrame with:\n    8 variable(s) (8 unique refs)\n    3 operation(s) (3 unique calls)\nComputational graph:\n    X_train@output_0, X_test@output_1, y_train@output_2, y_test@output_3 = \ngenerate_dataset(random_seed=random_seed)\n    model@output_0 = train_model(X_train=X_train, n_estimators=n_estimators, y_train=y_train)\n    v@output_0 = eval_model(X_test=X_test, model=model, y_test=y_test)\n</pre> <p></p> <p>The variables added to the CF during expansion are given informative names based on input names of functions called on these variables. The result of expansion tells us the precise way <code>test_acc</code> was computed. We can get the values of each variable and the calls of each function:</p> <pre><code>pprint({vname: storage.unwrap(refs) \n for vname, refs in cf.refs_by_var().items()\n if vname not in ['X_train', 'X_test', 'y_train', 'y_test'] # to save space\n })\n\npprint(cf.calls_by_func())\n</code></pre> <pre>{\n    'v': {0.82},\n    'model': {RandomForestClassifier(max_depth=2, n_estimators=80)},\n    'random_seed': {42},\n    'n_estimators': {80}\n}\n</pre> <pre>{\n    'eval_model': {Call(eval_model, hid=d32...)},\n    'generate_dataset': {Call(generate_dataset, hid=c3f...)},\n    'train_model': {Call(train_model, hid=e60...)}\n}\n</pre>"},{"location":"topics/03_cf/#from-all-calls-to-an-op","title":"From all calls to an <code>@op</code>","text":"<p>Another way to create a CF is to initialize is with all calls to a given <code>@op</code>:</p> <pre><code>cf = storage.cf(train_model)\npprint(cf)\ncf.draw(verbose=True)\n</code></pre> <pre>ComputationFrame with:\n    5 variable(s) (14 unique refs)\n    1 operation(s) (4 unique calls)\nComputational graph:\n    var_0@output_0, var_1@output_1 = train_model(X_train=X_train, n_estimators=n_estimators, y_train=y_train)\n</pre> <p></p> <p>You can extract a dataframe from any CF (explained more later); in particular, the dataframe for the CF of a single <code>@op</code> will be the memoization table for this <code>@op</code>:</p> <pre><code>print(cf.df(values='refs').to_markdown())\n</code></pre> <pre><code>|    | n_estimators                         | y_train                              | X_train                              | train_model                   | var_1                                | var_0                                |\n|---:|:-------------------------------------|:-------------------------------------|:-------------------------------------|:------------------------------|:-------------------------------------|:-------------------------------------|\n|  0 | AtomRef(hid=98c..., in_memory=False) | AtomRef(hid=faf..., in_memory=False) | AtomRef(hid=efa..., in_memory=False) | Call(train_model, hid=5f7...) | AtomRef(hid=760..., in_memory=False) | AtomRef(hid=b25..., in_memory=False) |\n|  1 | AtomRef(hid=235..., in_memory=False) | AtomRef(hid=faf..., in_memory=False) | AtomRef(hid=efa..., in_memory=False) | Call(train_model, hid=c55...) | AtomRef(hid=5b7..., in_memory=False) | AtomRef(hid=208..., in_memory=False) |\n|  2 | AtomRef(hid=9fd..., in_memory=False) | AtomRef(hid=faf..., in_memory=False) | AtomRef(hid=efa..., in_memory=False) | Call(train_model, hid=514...) | AtomRef(hid=784..., in_memory=False) | AtomRef(hid=331..., in_memory=False) |\n|  3 | AtomRef(hid=120..., in_memory=False) | AtomRef(hid=faf..., in_memory=False) | AtomRef(hid=efa..., in_memory=False) | Call(train_model, hid=e60...) | AtomRef(hid=646..., in_memory=False) | AtomRef(hid=522..., in_memory=False) |\n</code></pre>"},{"location":"topics/03_cf/#_1","title":"Query the Storage with ComputationFrames","text":""},{"location":"topics/03_cf/#from-groups-of-refs-to-use-as-variables","title":"From groups of <code>Ref</code>s to use as variables","text":"<p>You can also manually initialize variables of the CF by passing a dict where values are <code>Ref</code> iterables:</p> <pre><code>with storage: \n    models, test_accs = [], []\n    X_train, X_test, y_train, y_test = generate_dataset()\n    for n_estimators in [10, 20, 40, 80]:\n        model, train_acc = train_model(X_train, y_train, n_estimators=n_estimators)\n        models.append(model)\n        if storage.unwrap(train_acc) &gt; 0.8: # conditional execution\n            test_acc = eval_model(model, X_test, y_test)\n            test_accs.append(test_acc)\n</code></pre> <pre><code>cf = storage.cf({'model': models, 'test_acc': test_accs})\npprint(cf)\ncf.draw(verbose=True)\n</code></pre> <pre>ComputationFrame with:\n    2 variable(s) (6 unique refs)\n    0 operation(s) (0 unique calls)\nComputational graph:\n    model, test_acc\n</pre> <p></p> <p>Again, this is not interesting unless you <code>expand</code> back and/or forward. We can  illustrate by expanding only the <code>model</code> variable forward:</p> <pre><code>cf.expand_forward(varnames=['model'], inplace=True)\ncf.draw(verbose=True)\n</code></pre> <p></p> <p>The expansion algorithm figures out that the calls to <code>eval_model</code> we add should connect to the <code>test_acc</code> variable.</p>"},{"location":"topics/03_cf/#from-any-collection-of-calls","title":"From any collection of calls","text":"<p>TODO</p>"},{"location":"topics/03_cf/#exploring-storage-by-expanding-computationframes","title":"Exploring storage by expanding <code>ComputationFrame</code>s","text":"<p>Once a CF is created, you can add computational context to it by calling one of a few methods:</p> <pre><code>pprint('Docstring for `expand_back`:')\npprint(cf.expand_back.__doc__)\npprint('Docstring for `expand_forward`:')\npprint(cf.expand_forward.__doc__)\npprint('Docstring for `expand_all`:')\npprint(cf.expand_all.__doc__)\n</code></pre> <pre>Docstring for `expand_back`:\n</pre> <pre>\n        Join to the CF the calls that created all refs in the given variables\n        that currently do not have a connected creator call in the CF.\n\n        If such refs are found, this will result to the addition of \n        - new function nodes for the calls that created these refs;\n        - new variable nodes for the *inputs* of these calls.\n\n        The number of these nodes and how they connect to the CF will depend on\n        the structure of the calls that created the refs. \n\n        Arguments:\n        - `varnames`: the names of the variables to expand; if None, expand all\n        the `Ref`s that don't have a creator call in any function node of the CF\n        that is connected to the `Ref`'s variable node as an output.\n        - `recursive`: if True, keep expanding until a fixed point is reached\n\n</pre> <pre>Docstring for `expand_forward`:\n</pre> <pre>\n        Join the calls that consume the given variables; see `expand_back` (the \n        dual operation) for more details.\n\n</pre> <pre>Docstring for `expand_all`:\n</pre> <pre>\n        Expand the computation frame by repeatedly applying `expand_back` and\n        `expand_forward` until a fixed point is reached.\n\n</pre>"},{"location":"topics/03_cf/#selective-expansion-with-expand_back-expand_forward","title":"Selective expansion with <code>expand_back</code>, <code>expand_forward</code>","text":"<p>You can be very precise about which calls to add to the CF. For example, you can expand variables one by one. When this results in convergent histories, the CF will detect this and reuse the calls:  </p> <pre><code>cf = storage.cf(test_acc)\ncf = cf.expand_back('v')\ncf.draw(verbose=True)\n</code></pre> <p></p> <pre><code>cf = cf.expand_back('X_test')\ncf.draw(verbose=True)\n</code></pre> <p></p> <pre><code>cf = cf.expand_back('model')\ncf.draw(verbose=True)\n</code></pre> <p></p> <pre><code>cf = cf.expand_back('X_train')\ncf.draw(verbose=True)\n</code></pre> <p></p> <p>When we expand <code>X_train</code> at the end, the expansion algorithm detects that the <code>generate_dataset</code> node should be reused instead of creating a new node.</p>"},{"location":"topics/03_cf/#full-expansion-with-expand_all","title":"Full expansion with <code>expand_all</code>","text":"<p>The easiest way to add all the calls that can be reached from a CF by calling <code>expand_back</code> or <code>expand_forward</code> is using <code>expand_all</code></p> <pre><code>cf = storage.cf(train_model).expand_all()\ncf.draw(verbose=True)\n</code></pre> <p></p> <p>This adds all the calls in the storage, because they're all reachable from some call to <code>train_model</code> by following inputs/outputs.</p>"},{"location":"topics/03_cf/#extracting-dataframes-from-computationframes","title":"Extracting <code>DataFrame</code>s from <code>ComputationFrame</code>s","text":""},{"location":"topics/03_cf/#computationframes-as-generalized-dataframes","title":"<code>ComputationFrame</code>s as generalized dataframes","text":"<p>You can think of <code>ComputationFrame</code>s as a generalization the familiar <code>pandas.DataFrame</code> class:</p> <ul> <li>instead of columns, you have a computational graph: functions whose input/output edges connect to variables.</li> <li>instead of rows, you have computation traces: variable values and function calls that (possibly partially) follow this graph</li> </ul> <p>Conversely, a dataframe can be extracted from any computation frame for easier later analysis:</p> <ul> <li>the columns are the nodes in the graph (functions and variables)</li> <li>each row is a computation trace, possibly padded with <code>NaN</code>s where no value/call is present.</li> </ul>"},{"location":"topics/03_cf/#the-df-method","title":"The <code>.df()</code> method","text":"<p>All ways to turn a CF into a DF are dispatched through a CF's <code>.df()</code> method. We can apply this to the full storage CF we computed last:</p> <pre><code>cf = storage.cf(train_model).expand_all()\nprint(cf.df().drop(columns=['X_train', 'y_train']).to_markdown())\n</code></pre> <pre><code>|    |   random_seed | generate_dataset                   |   n_estimators | train_model                   |   var_1 | var_0                                                | eval_model                   |   var_2 |\n|---:|--------------:|:-----------------------------------|---------------:|:------------------------------|--------:|:-----------------------------------------------------|:-----------------------------|--------:|\n|  0 |            42 | Call(generate_dataset, hid=c3f...) |             10 | Call(train_model, hid=5f7...) |    0.74 | RandomForestClassifier(max_depth=2, n_estimators=10) |                              |  nan    |\n|  1 |            42 | Call(generate_dataset, hid=c3f...) |             20 | Call(train_model, hid=c55...) |    0.8  | RandomForestClassifier(max_depth=2, n_estimators=20) |                              |  nan    |\n|  2 |            42 | Call(generate_dataset, hid=c3f...) |             40 | Call(train_model, hid=514...) |    0.82 | RandomForestClassifier(max_depth=2, n_estimators=40) | Call(eval_model, hid=5d3...) |    0.81 |\n|  3 |            42 | Call(generate_dataset, hid=c3f...) |             80 | Call(train_model, hid=e60...) |    0.83 | RandomForestClassifier(max_depth=2, n_estimators=80) | Call(eval_model, hid=d32...) |    0.82 |\n</code></pre> <p>Importantly, we see that some computations only partially follow the full computation graph, because we didn't call <code>eval_model</code> on all the <code>train_model</code> outputs.</p>"},{"location":"topics/03_cf/#what-does-df-actually-compute","title":"What does <code>.df()</code> actually compute?","text":"<p>The <code>.df</code> method does roughly speaking the following:</p> <ul> <li>finds all the sink, i.e. \"final\", <code>Ref</code>s in the CF, i.e. the ones that are not used as inputs to any call in a connected function node.</li> <li>for each variable's sink <code>Ref</code>s, it computes a table where <ul> <li>columns are variables and functions that these <code>Ref</code>s depend on</li> <li>rows contain the set of <code>Ref</code>/<code>Call</code> dependencies from each such node</li> </ul> </li> <li>joins these tables over all the variables containing sink <code>Ref</code>s along their shared columns</li> </ul>"},{"location":"topics/03_cf/#visualizing-and-summarizing-computationframes","title":"Visualizing and summarizing <code>ComputationFrame</code>s","text":"<p>CFs are complex data structures, and benefit from several kinds of high-level summaries.</p>"},{"location":"topics/03_cf/#the-draw-method","title":"The <code>.draw()</code> method","text":"<p>We've used the <code>.draw()</code> method a bunch of times so far. Let's revisit the most recent CF:</p> <pre><code>cf.draw(verbose=True)\n</code></pre> <p></p> <p>The <code>.draw()</code> method shows the computational graph of a CF and how many calls/refs there are in each function/variable. When called with <code>verbose=True</code>, this method shows a summary of how the variable values and function calls connect across the CF:</p> <ul> <li>source (i.e. initial) <code>Ref</code>s in each variable: the <code>Ref</code>s in this variable that aren't outputs of any call in a function node using the variable as output.</li> <li>sink (i.e. final) <code>Ref</code>s in each variable: dual to source refs, these are the <code>Ref</code>s that are not inputs to calls in any function node using the variable as input.</li> <li>edge <code>Ref</code>s for each edge: for edges that are inputs to functions, this is the number of refs from the input variable that are used in calls to the function. For edges that are outputs of functions, this is the number of refs in the output variable created by calls in the function node.</li> </ul>"},{"location":"topics/03_cf/#printing-the-computation-graph-of-a-computationframe","title":"Printing the computation graph of a <code>ComputationFrame</code>","text":"<p>The computational graph of a CF is part of its <code>repr()</code>, but you can also print just the graph:</p> <pre><code>cf.print_graph()\n</code></pre> <pre><code>X_train@output_0, y_train@output_2 = generate_dataset(random_seed=random_seed)\nvar_0@output_0, var_1@output_1 = train_model(X_train=X_train, n_estimators=n_estimators, y_train=y_train)\nvar_2@output_0 = eval_model(model=var_0)\n</code></pre> <p>Note that outputs are appended with <code>@output_0</code>, .... This is to indicate the name each output is bound to, in case some outputs are missing from the graph (which is possible, because CFs can represent partial computations).</p>"},{"location":"topics/04_versions/","title":"Changing <code>@op</code>s and managing versions","text":"<p>It should be easy to change your code and have the storage respond in a correct way (e.g., recompute a call only when the logic behind it has changed). <code>mandala</code> provides the following mechanisms to do that:</p> <ul> <li>automatic per-call dependency tracking: every <code>@op</code> call records the functions it called along the way. This allows the <code>storage</code> to automatically know, given some inputs, whether a past call for these inputs can be reused  given the current state of the code. This is a very fine-grained notion of reuse.</li> <li>marking changes as breaking vs non-breaking: when a change to an <code>@op</code> or its dependencies is detected, you can choose to mark it as breaking the calls that depend on it or not. However, breaking changes are generally more fool-proof; see caveats of non-breaking changes.</li> <li>content-based versioning: the current state of the codebase uniquely determines the version each <code>@op</code> is in. There are no arbitrary names attached to versions. The versions for each <code>@op</code> can be inspected in a <code>git</code>-like data structure.</li> </ul>"},{"location":"topics/04_versions/#enabling-and-configuring-versioning","title":"Enabling and configuring versioning","text":"<p>Passing a value to the <code>deps_path</code> parameter of the <code>Storage</code> class enables dependency tracking and versioning. This means that any time a memoized function actually executes (instead of reusing a past call's results), it keeps track of the functions and global variables it accesses along the way. </p> <p>Usually, the functions we want to track are limited to user-defined ones (you typically don't want to track changes in installed libraries!):</p> <ul> <li>Setting <code>deps_path</code> to <code>\"__main__\"</code> will only look for dependencies <code>f</code> defined in the current interactive session or process (as determined by <code>f.__module__</code>).</li> <li>Setting it to a folder will only look for dependencies defined in this folder.</li> </ul> <pre><code># for Google Colab\ntry:\n    import google.colab\n    !pip install git+https://github.com/amakelov/mandala\nexcept:\n    pass\n</code></pre> <pre><code>from mandala.imports import Storage, op, track\n\nstorage = Storage(deps_path='__main__')\n</code></pre>"},{"location":"topics/04_versions/#the-track-decorator","title":"The <code>@track</code> decorator","text":"<p>The most efficient and reliable implementation of dependency tracking currently requires you to explicitly put <code>@track</code> on non-memoized functions and classes you want to track. This limitation may be lifted in the future, but at the cost of more magic (i.e., automatically applying the decorator to functions in the current local scope that originate in given paths).</p> <p>The alternative (experimental) decorator implementation is based on <code>sys.settrace</code>. Limitations are described in this blog post)</p>"},{"location":"topics/04_versions/#examining-the-captured-versions","title":"Examining the captured versions","text":"<p>Let's run a small ML pipeline, where we optionally apply scaling to the data, introducing a non-<code>@op</code> dependency for some of the calls:</p> <pre><code>from sklearn.datasets import load_digits\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nN_CLASS = 10\n\n@track # to track a non-memoized function as a dependency\ndef scale_data(X):\n    return StandardScaler(with_mean=True, with_std=False).fit_transform(X)\n\n@op\ndef load_data():\n    X, y = load_digits(n_class=N_CLASS, return_X_y=True)\n    return X, y\n\n@op\ndef train_model(X, y, scale=False):\n    if scale:\n        X = scale_data(X)\n    return LogisticRegression(max_iter=1000, solver='liblinear').fit(X, y)\n\n@op\ndef eval_model(model, X, y, scale=False):\n    if scale:\n        X = scale_data(X)\n    return model.score(X, y)\n\nwith storage:\n    X, y = load_data()\n    for scale in [False, True]:\n        model = train_model(X, y, scale=scale)\n        acc = eval_model(model, X, y, scale=scale)\n</code></pre> <p>Now <code>train_model</code> and <code>eval_model</code> will each have two versions - one that depends on <code>scale_data</code> and one that doesn't. You can confirm this by calling e.g. <code>storage.versions(train_model)</code>:</p> <pre><code>storage.versions(train_model)\n</code></pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ### Dependencies for version of function train_model from module __main__                                       \u2502\n\u2502 ### content_version_id=db93a1e9c60fb37868575845a7afe47d                                                         \u2502\n\u2502 ### semantic_version_id=2acaa8919ddd4b5d8846f1f2d15bc971                                                        \u2502\n\u2502                                                                                                                 \u2502\n\u2502 ################################################################################                                \u2502\n\u2502 ### IN MODULE \"__main__\"                                                                                        \u2502\n\u2502 ################################################################################                                \u2502\n\u2502                                                                                                                 \u2502\n\u2502 @op                                                                                                             \u2502\n\u2502 def train_model(X, y, scale=False):                                                                             \u2502\n\u2502     if scale:                                                                                                   \u2502\n\u2502         X = scale_data(X)                                                                                       \u2502\n\u2502     return LogisticRegression(max_iter=1000, solver='liblinear').fit(X, y)                                      \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ### Dependencies for version of function train_model from module __main__                                       \u2502\n\u2502 ### content_version_id=2717c55fbbbb60442535a8dea0c81f67                                                         \u2502\n\u2502 ### semantic_version_id=4674057d19bbf217687dd9dabe01df36                                                        \u2502\n\u2502                                                                                                                 \u2502\n\u2502 ################################################################################                                \u2502\n\u2502 ### IN MODULE \"__main__\"                                                                                        \u2502\n\u2502 ################################################################################                                \u2502\n\u2502                                                                                                                 \u2502\n\u2502 @track # to track a non-memoized function as a dependency                                                       \u2502\n\u2502 def scale_data(X):                                                                                              \u2502\n\u2502     return StandardScaler(with_mean=True, with_std=False).fit_transform(X)                                      \u2502\n\u2502                                                                                                                 \u2502\n\u2502 @op                                                                                                             \u2502\n\u2502 def train_model(X, y, scale=False):                                                                             \u2502\n\u2502     if scale:                                                                                                   \u2502\n\u2502         X = scale_data(X)                                                                                       \u2502\n\u2502     return LogisticRegression(max_iter=1000, solver='liblinear').fit(X, y)                                      \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre>"},{"location":"topics/04_versions/#making-changes-and-sorting-them-into-breaking-and-non-breaking","title":"Making changes and sorting them into breaking and non-breaking","text":"<p>Now suppose we make some changes and re-run:</p> <ul> <li>we change the value of the global variable <code>N_CLASS</code>;</li> <li>we change the code of <code>scale_data</code> in a semantically meaningful (i.e., breaking) way</li> <li>we change the code of <code>eval_model</code> in a \"cosmetic\" way that can be considered non-breaking.</li> </ul> <p>When entering the <code>storage</code> block, the storage will detect the changes in the tracked components, and for each change will present you with the functions affected:</p> <ul> <li><code>N_CLASS</code> is a dependency for <code>load_data</code>;</li> <li><code>scale_data</code> is a dependency for the calls to <code>train_model</code> and <code>eval_model</code>   which had <code>scale=True</code>;</li> <li><code>eval_model</code> is a dependency for itself.</li> </ul> <pre><code>from mandala.utils import mock_input\nfrom unittest.mock import patch\n\nN_CLASS = 5\n\n@track\ndef scale_data(X):\n    return StandardScaler(with_mean=True, with_std=True).fit_transform(X)\n\n@op\ndef eval_model(model, X, y, scale=False):\n    if scale:\n        X = scale_data(X)\n    return round(model.score(X, y), 2)\n\nanswers = ['y', 'n', 'y']\n\nwith patch('builtins.input', mock_input(answers)):\n    with storage:\n        X, y = load_data()\n        for scale in [False, True]:\n            model = train_model(X, y, scale=scale)\n            acc = eval_model(model, X, y, scale=scale)\n</code></pre> <pre><code>CHANGE DETECTED in N_CLASS from module __main__\nDependent components:\n  Version of \"load_data\" from module \"__main__\" (content: 426c4c8c56d7c0d6374095c7d4a4974f, semantic: 7d0732b9bfb31e5e2211e0122651a624)\n</code></pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Diff \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   1 -10                                                                                                         \u2502\n\u2502   2 +5                                                                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre><code>Does this change require recomputation of dependent calls?\nWARNING: if the change created new dependencies and you choose 'no', you should add them by hand or risk missing changes in them.\nAnswer: [y]es/[n]o/[a]bort \nYou answered: \"y\"\nCHANGE DETECTED in eval_model from module __main__\nDependent components:\n  Version of \"eval_model\" from module \"__main__\" (content: 955b2a683de8dacf624047c0e020140a, semantic: c847d6dc3f23c176e6c8bf9e7006576a)\n  Version of \"eval_model\" from module \"__main__\" (content: 5bdcd6ffc4888990d8922aa85795198d, semantic: 4e1d702e9797ebba156831294de46425)\n</code></pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Diff \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   1      if scale:                                                                                              \u2502\n\u2502   2          X = scale_data(X)                                                                                  \u2502\n\u2502   3 -    return model.score(X, y)                                                                               \u2502\n\u2502   4 +    return round(model.score(X, y), 2)                                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre><code>Does this change require recomputation of dependent calls?\nWARNING: if the change created new dependencies and you choose 'no', you should add them by hand or risk missing changes in them.\nAnswer: [y]es/[n]o/[a]bort \nYou answered: \"n\"\nCHANGE DETECTED in scale_data from module __main__\nDependent components:\n  Version of \"train_model\" from module \"__main__\" (content: 2717c55fbbbb60442535a8dea0c81f67, semantic: 4674057d19bbf217687dd9dabe01df36)\n  Version of \"eval_model\" from module \"__main__\" (content: 5bdcd6ffc4888990d8922aa85795198d, semantic: 4e1d702e9797ebba156831294de46425)\n</code></pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Diff \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   1 -@track # to track a non-memoized function as a dependency                                                  \u2502\n\u2502   2 +@track                                                                                                     \u2502\n\u2502   3  def scale_data(X):                                                                                         \u2502\n\u2502   4 -    return StandardScaler(with_mean=True, with_std=False).fit_transform(X)                                 \u2502\n\u2502   5 +    return StandardScaler(with_mean=True, with_std=True).fit_transform(X)                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre><code>Does this change require recomputation of dependent calls?\nWARNING: if the change created new dependencies and you choose 'no', you should add them by hand or risk missing changes in them.\nAnswer: [y]es/[n]o/[a]bort \nYou answered: \"y\"\n</code></pre> <p>When a change is detected, the UI:</p> <ul> <li>shows the diffs in each function,</li> <li>gives you a list of which <code>@op</code>s' versions are affected by each change</li> <li>lets you choose if the change is breaking or non-breaking</li> </ul> <p>We can check what happened by constructing a computation frame:</p> <pre><code>cf = storage.cf(eval_model).expand_all()\ncf.draw(verbose=True)\n</code></pre> <p></p> <p>We see that <code>load_data</code> has two versions in use, whereas <code>train_model</code> and <code>eval_model</code> both have three. Which ones? Again, call <code>versions</code> to find out. For example, with <code>eval_model</code>, we have 4 different content versions, that  overall span 3 semantically different versions:</p> <pre><code>storage.versions(eval_model)\n</code></pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ### Dependencies for version of function eval_model from module __main__                                        \u2502\n\u2502 ### content_version_id=955b2a683de8dacf624047c0e020140a                                                         \u2502\n\u2502 ### semantic_version_id=c847d6dc3f23c176e6c8bf9e7006576a                                                        \u2502\n\u2502                                                                                                                 \u2502\n\u2502 ################################################################################                                \u2502\n\u2502 ### IN MODULE \"__main__\"                                                                                        \u2502\n\u2502 ################################################################################                                \u2502\n\u2502                                                                                                                 \u2502\n\u2502 @op                                                                                                             \u2502\n\u2502 def eval_model(model, X, y, scale=False):                                                                       \u2502\n\u2502     if scale:                                                                                                   \u2502\n\u2502         X = scale_data(X)                                                                                       \u2502\n\u2502     return model.score(X, y)                                                                                    \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ### Dependencies for version of function eval_model from module __main__                                        \u2502\n\u2502 ### content_version_id=5bdcd6ffc4888990d8922aa85795198d                                                         \u2502\n\u2502 ### semantic_version_id=4e1d702e9797ebba156831294de46425                                                        \u2502\n\u2502                                                                                                                 \u2502\n\u2502 ################################################################################                                \u2502\n\u2502 ### IN MODULE \"__main__\"                                                                                        \u2502\n\u2502 ################################################################################                                \u2502\n\u2502                                                                                                                 \u2502\n\u2502 @op                                                                                                             \u2502\n\u2502 def eval_model(model, X, y, scale=False):                                                                       \u2502\n\u2502     if scale:                                                                                                   \u2502\n\u2502         X = scale_data(X)                                                                                       \u2502\n\u2502     return model.score(X, y)                                                                                    \u2502\n\u2502                                                                                                                 \u2502\n\u2502 @track # to track a non-memoized function as a dependency                                                       \u2502\n\u2502 def scale_data(X):                                                                                              \u2502\n\u2502     return StandardScaler(with_mean=True, with_std=False).fit_transform(X)                                      \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ### Dependencies for version of function eval_model from module __main__                                        \u2502\n\u2502 ### content_version_id=b50e3e2529b811e226d2bb39a572a5e4                                                         \u2502\n\u2502 ### semantic_version_id=c847d6dc3f23c176e6c8bf9e7006576a                                                        \u2502\n\u2502                                                                                                                 \u2502\n\u2502 ################################################################################                                \u2502\n\u2502 ### IN MODULE \"__main__\"                                                                                        \u2502\n\u2502 ################################################################################                                \u2502\n\u2502                                                                                                                 \u2502\n\u2502 @op                                                                                                             \u2502\n\u2502 def eval_model(model, X, y, scale=False):                                                                       \u2502\n\u2502     if scale:                                                                                                   \u2502\n\u2502         X = scale_data(X)                                                                                       \u2502\n\u2502     return model.score(X, y)                                                                                    \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ### Dependencies for version of function eval_model from module __main__                                        \u2502\n\u2502 ### content_version_id=136129b20d9a3a3795e88ba8cf89b115                                                         \u2502\n\u2502 ### semantic_version_id=f2573de2a6c25b390fc86d665ea85687                                                        \u2502\n\u2502                                                                                                                 \u2502\n\u2502 ################################################################################                                \u2502\n\u2502 ### IN MODULE \"__main__\"                                                                                        \u2502\n\u2502 ################################################################################                                \u2502\n\u2502                                                                                                                 \u2502\n\u2502 @op                                                                                                             \u2502\n\u2502 def eval_model(model, X, y, scale=False):                                                                       \u2502\n\u2502     if scale:                                                                                                   \u2502\n\u2502         X = scale_data(X)                                                                                       \u2502\n\u2502     return model.score(X, y)                                                                                    \u2502\n\u2502                                                                                                                 \u2502\n\u2502 @track                                                                                                          \u2502\n\u2502 def scale_data(X):                                                                                              \u2502\n\u2502     return StandardScaler(with_mean=True, with_std=True).fit_transform(X)                                       \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre>"},{"location":"topics/04_versions/#additional-notes","title":"Additional notes","text":""},{"location":"topics/04_versions/#so-what-really-is-a-version-of-an-op","title":"So what really is a version of an <code>@op</code>?","text":"<p>A version for an <code>@op</code> is a collection of</p> <ul> <li>(hashes of) the source code of functions and methods</li> <li>(hashes of) values of global variables</li> </ul> <p>at the time when a call to this <code>@op</code> was executed. Even if you don't change anything in the code, a single function can have multiple versions if it invokes different dependencies for different calls. </p>"},{"location":"topics/04_versions/#going-back-in-time","title":"Going back in time","text":"<p>Since the versioning system is content-based, simply restoring an old state of the code makes the storage automatically recognize which \"world\" it's in, and which calls are memoized in this world.</p>"},{"location":"topics/04_versions/#caveats-of-marking-changes-as-non-breaking","title":"Caveats of marking changes as non-breaking","text":"<p>The main motivation for allowing non-breaking changes is to maintain the storage when doing routine code improvements (refactoring, comments, logging).</p> <p>However, non-semantic changes should be applied with care. Apart from being prone to errors (you wrongly conclude that a change has no effect on semantics when it does), they can also introduce invisible dependencies: suppose you factor a function out of some dependency and mark the change non-semantic. Then the newly extracted function may in reality be a dependency of the existing calls, but this goes unnoticed by the system. Consequently, changes in this  dependency may go unnoticed by the versioning algorithm.</p>"},{"location":"topics/05_collections/","title":"Natively handling Python collections","text":"<p>A key benefit of <code>mandala</code> over straightforward memoization is that it can make Python collections (lists, dicts, ...) a native &amp; transparent part of the memoization process:</p> <ul> <li><code>@op</code>s can return collections where each item is a separate <code>Ref</code>, so that later <code>@op</code> calls can work with individual elements;</li> <li><code>@op</code>s can also accept as input collections where each item is a separate <code>Ref</code>, to e.g. implement aggregation operations over them.</li> <li>collections can reuse the storage of their items: if two collections share some elements, each shared element is stored only once in storage.</li> <li>the relationship between a collection and each of its items is a native part of the computational graph of <code>@op</code> calls, and can be propagated automatically by <code>ComputationFrame</code>s. Indeed, collections are implemented as <code>@op</code>s internally.</li> </ul>"},{"location":"topics/05_collections/#inputoutput-collections-must-be-explicitly-annotated","title":"Input/output collections must be explicitly annotated","text":"<p>By default, any collection passed as <code>@op</code> input or output will be stored as a single <code>Ref</code> with no structure; the object is opaque to the <code>Storage</code> instance. To make the collection transparent to the <code>Storage</code>, you must override this behavior explicitly by using a custom type annotation, such as <code>MList</code> for lists, <code>MDict</code> for dicts, ...:</p> <pre><code># for Google Colab\ntry:\n    import google.colab\n    !pip install git+https://github.com/amakelov/mandala\nexcept:\n    pass\n</code></pre> <pre><code>from mandala.imports import Storage, op, MList\n\nstorage = Storage()\n\n@op\ndef average(nums: MList[float]) -&gt; float:\n    return sum(nums) / len(nums)\n\nwith storage:\n    a = average([1, 2, 3, 4, 5])\n</code></pre> <p>We can understand how the list was made transparent to the storage by inspecting the computation frame:</p> <pre><code>cf = storage.cf(average).expand_all(); cf.draw(verbose=True, orientation='LR')\n</code></pre> <p></p> <p>We see that the internal <code>__make_list__</code> operation was automatically applied to create a list, which is then the <code>Ref</code> passed to <code>average</code>. </p>"},{"location":"topics/05_collections/#how-collections-interact-with-computationframes","title":"How collections interact with <code>ComputationFrame</code>s","text":"<p>In general, CFs are turned into dataframes that capture the joint history of the final <code>Ref</code>s in the CF. When there are collection <code>@op</code>s in the CF, a single <code>Ref</code> (such as the element of <code>nums</code> above) can depend on multiple <code>Ref</code>s in another variable (such as the <code>Ref</code>s in the <code>elts</code> variable).</p> <p>We can observe this by taking the dataframe of the above CF:</p> <pre><code>print(cf.df(values='objs').to_markdown())\n</code></pre> <pre><code>|    | elts                             | __make_list__                   | nums            | average                   |   var_0 |\n|---:|:---------------------------------|:--------------------------------|:----------------|:--------------------------|--------:|\n|  0 | ValueCollection([2, 4, 1, 3, 5]) | Call(__make_list__, hid=172...) | [1, 2, 3, 4, 5] | Call(average, hid=38e...) |       3 |\n</code></pre> <p>There's only a single row, but in the <code>elts</code> column we see a <code>ValueCollection</code> object, indicating that there are multiple <code>Ref</code>s in <code>elts</code> that are dependencies of <code>output_0</code>.</p>"},{"location":"topics/06_advanced_cf/","title":"Advanced <code>ComputationFrame</code> tools","text":"<p>This section of the documentation contains some more advanced <code>ComputationFrame</code> topics.</p>"},{"location":"topics/06_advanced_cf/#set-like-and-graph-like-operations-on-computationframes","title":"Set-like and graph-like operations on <code>ComputationFrame</code>s","text":"<p>A CF is like a \"graph of sets\", where the elements of each set are either <code>Ref</code>s (for variables) or <code>Call</code>s (for functions). As such, it supports both natural set-like operations applied node/edge-wise, and natural operations using the graph's connectivity:</p> <ul> <li>union: given by <code>cf_1 | cf_2</code>, this takes the union of the two computation graphs (merging nodes/edges with the same names), and in case of a merge, the resulting set at the node is the union of the two sets of <code>Ref</code>s or <code>Call</code>s.</li> <li>intersection: given by <code>cf_1 &amp; cf_2</code>, this takes the intersection of the two computation graphs (leaving only nodes/edges with the same name in both),  and the set at each node is the intersection of the two corresponding sets. </li> <li><code>.downstream(varnames)</code>: restrict the CF to computations that are downstream of the <code>Ref</code>s in chosen variables</li> <li><code>.upstream(varnames)</code>: dual to <code>downstream</code></li> </ul> <p>Consider the following example:</p> <pre><code># for Google Colab\ntry:\n    import google.colab\n    !pip install git+https://github.com/amakelov/mandala\nexcept:\n    pass\n</code></pre> <pre><code>from mandala.imports import *\nstorage = Storage()\n\n@op\ndef inc(x): return x + 1\n\n@op\ndef add(y, z): return y + z\n\n@op\ndef square(w): return w ** 2\n\n@op\ndef divmod_(u, v): return divmod(u, v)\n\nwith storage:\n    xs = [inc(i) for i in range(5)]\n    ys = [add(x, z=42) for x in xs] + [square(x) for x in range(5, 10)]\n    zs = [divmod_(x, y) for x, y in zip(xs, ys[3:8])]\n</code></pre> <p>We have a \"middle layer\" in the computation that uses both <code>add</code> and <code>square</code>. We can get a shared view of the entire computation by taking the union of the expanded CFs for these two ops:</p> <pre><code>cf = (storage.cf(add) | storage.cf(square)).expand_all()\ncf.draw(verbose=True)\n</code></pre> <p></p>"},{"location":"topics/06_advanced_cf/#selection","title":"Selection","text":"<p>TODO</p>"},{"location":"tutorials/01_hello/","title":"Quickstart","text":"<p><code>mandala</code> eliminates the developer effort typically required to persist, iterate on, query, version and reproduce results of computational projects, such as machine learning experiments. </p> <p>It works by automatically capturing inputs, outputs, and code (+dependencies) at calls of <code>@op</code>-decorated functions. A <code>ComputationFrame</code> data structure over this information enables easy queries and high-level operations over program traces.</p> <pre><code># for Google Colab\ntry:\n    import google.colab\n    !pip install git+https://github.com/amakelov/mandala\nexcept:\n    pass# Run this if in a Google Colab notebook\n</code></pre>"},{"location":"tutorials/01_hello/#the-op-decorator-automatic-memoization-and-code-tracking","title":"The <code>@op</code> decorator: automatic memoization and code tracking","text":"<p><code>@op</code> tracks the inputs, outputs, code and dependencies of calls to Python functions. The same call is never executed twice:</p> <pre><code>from mandala.imports import *\nimport time\n\nstorage = Storage( # stores all `@op` calls\n    # where to look for dependencies; use `None` to prevent versioning altogether\n    deps_path='__main__' \n    ) \n\n@op\ndef inc(x):\n    print(\"Hello from inc!\")\n    time.sleep(1) # simulate a long operation\n    return x + 1\n\nwith storage: # all `@op` calls inside this block will be stored in `storage`\n    start = time.time()\n    a = inc(1)\n    b = inc(1) # this will not be executed, but reused\n    end = time.time()\n    print(f'Took {round(end - start)} seconds')\n</code></pre> <pre><code>Hello from inc!\nTook 1 seconds\n</code></pre>"},{"location":"tutorials/01_hello/#computationframes-generalized-dataframes-for-querying-saved-computations","title":"<code>ComputationFrame</code>s: generalized dataframes for querying saved computations","text":"<p><code>@op</code>s are designed to be composed with one another like ordinary Python functions. This automatically keeps track of the relationships between all saved objects. </p> <p>The <code>ComputationFrame</code> data structure is a natural high-level view of these relationships that can be used to explore storage and extract computation traces in a format useful for analysis. It groups together saved <code>@op</code> calls into computational graphs: </p> <pre><code>@op # define a new @op to compose with `inc`\ndef add(x, y):\n    print(\"Hello from add!\")\n    return x + y\n\nwith storage:\n    for i in range(5):\n        j = inc(i)\n        if i % 2 == 0:\n            k = add(i, j)\n\n# get &amp; visualize the computation frame for all calls to `inc`\ncf = storage.cf(inc) \nprint('Computation frame for `inc`:')\ncf.draw(verbose=True, orientation='LR') # visualize the computation frame\n\n# expand the computation frame to include all calls connected to the calls of\n# `inc` through shared inputs/outputs\ncf.expand_all(inplace=True) \nprint('Expanded computation frame for `inc`:')\ncf.draw(verbose=True, orientation='LR', path='test.jpg') # visualize the computation frame\n</code></pre> <pre><code>Hello from inc!\nHello from add!\nHello from inc!\nHello from add!\nHello from inc!\nHello from inc!\nHello from add!\nComputation frame for `inc`:\n</code></pre> <p></p> <pre><code>Expanded computation frame for `inc`:\n</code></pre> <p></p>"},{"location":"tutorials/01_hello/#computation-frames-generalize-dataframes-to-operate-over-computation-traces","title":"Computation frames generalize dataframes to operate over computation traces","text":"<ul> <li>columns are replaced by a computational graph: functions whose input/output edges connect to variables.</li> <li>rows are replaced by computation traces: variable values and function calls that (possibly partially) follow this graph</li> </ul> <p>A dataframe can be extracted from any computation frame for easier later analysis: - the columns are the nodes in the graph (functions and variables) - each row is a computation trace, possibly padded with <code>NaN</code>s where no value/call is present:</p> <pre><code>print(cf.df().to_markdown())\n</code></pre> <pre><code>|    |   x | inc                   |   var_0 | add                   |   var_1 |\n|---:|----:|:----------------------|--------:|:----------------------|--------:|\n|  0 |   3 | Call(inc, hid=f62...) |       4 |                       |     nan |\n|  1 |   2 | Call(inc, hid=ec7...) |       3 | Call(add, hid=d3f...) |       5 |\n|  2 |   4 | Call(inc, hid=f05...) |       5 | Call(add, hid=5f0...) |       9 |\n|  3 |   0 | Call(inc, hid=52f...) |       1 | Call(add, hid=38e...) |       1 |\n|  4 |   1 | Call(inc, hid=66c...) |       2 |                       |     nan |\n</code></pre>"},{"location":"tutorials/01_hello/#automatic-per-call-versioning-w-dependency-tracking","title":"Automatic per-call versioning w/ dependency tracking","text":"<p>Changing memoized functions may invalidate their past calls - but not all changes invalidate all calls, and some \"non-semantic\" changes invalidate no calls at all. </p> <p>To help with that, <code>mandala</code> can automatically track marked (with <code>@track</code>) dependencies of each call to an <code>@op</code>, and watch for changes in their code:</p> <pre><code>from unittest.mock import patch\nfrom mandala.utils import mock_input # to simulate user input non-interactively\n\n@op # define a new @op to compose with `inc`\ndef add(x, y):\n    print(\"Hello from add!\")\n    return x + square(y)\n\n@track # dependency tracking decorator\ndef square(num):\n    return num**2\n\n# same computations as before, change to `add` will be detected\nwith patch('builtins.input', mock_input(['y'])):\n    with storage:\n        for i in range(5):\n            j = inc(i)\n            if i % 2 == 0:\n                k = add(i, j)\n</code></pre> <pre><code>CHANGE DETECTED in add from module __main__\nDependent components:\n  Version of \"add\" from module \"__main__\" (content: 7cd06a0178abc60d137bb47bceafa5f9, semantic: 455b6b8789fb67940e41dbbb135292f7)\n</code></pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Diff \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   1  def add(x, y):                                                                                             \u2502\n\u2502   2      print(\"Hello from add!\")                                                                               \u2502\n\u2502   3 -    return x + y                                                                                           \u2502\n\u2502   4 +    return x + square(y)                                                                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre><code>Does this change require recomputation of dependent calls?\nWARNING: if the change created new dependencies and you choose 'no', you should add them by hand or risk missing changes in them.\nAnswer: [y]es/[n]o/[a]bort \nYou answered: \"y\"\nHello from add!\nHello from add!\nHello from add!\n</code></pre> <p>Now we've created a new, semantically distinct version of <code>add</code>. The versions and their dependencies can be inspected with the <code>.versions</code> method:</p> <pre><code>storage.versions(add)\n</code></pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ### Dependencies for version of function add from module __main__                                               \u2502\n\u2502 ### content_version_id=7cd06a0178abc60d137bb47bceafa5f9                                                         \u2502\n\u2502 ### semantic_version_id=455b6b8789fb67940e41dbbb135292f7                                                        \u2502\n\u2502                                                                                                                 \u2502\n\u2502 ################################################################################                                \u2502\n\u2502 ### IN MODULE \"__main__\"                                                                                        \u2502\n\u2502 ################################################################################                                \u2502\n\u2502                                                                                                                 \u2502\n\u2502 @op # define a new @op to compose with `inc`                                                                    \u2502\n\u2502 def add(x, y):                                                                                                  \u2502\n\u2502     print(\"Hello from add!\")                                                                                    \u2502\n\u2502     return x + y                                                                                                \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ### Dependencies for version of function add from module __main__                                               \u2502\n\u2502 ### content_version_id=6001cb6bf4c98e8a1b1a2f9170c7dd14                                                         \u2502\n\u2502 ### semantic_version_id=d1bae9c7d7f59e37d04dcb80adc06138                                                        \u2502\n\u2502                                                                                                                 \u2502\n\u2502 ################################################################################                                \u2502\n\u2502 ### IN MODULE \"__main__\"                                                                                        \u2502\n\u2502 ################################################################################                                \u2502\n\u2502                                                                                                                 \u2502\n\u2502 @op # define a new @op to compose with `inc`                                                                    \u2502\n\u2502 def add(x, y):                                                                                                  \u2502\n\u2502     print(\"Hello from add!\")                                                                                    \u2502\n\u2502     return x + square(y)                                                                                        \u2502\n\u2502                                                                                                                 \u2502\n\u2502 @track # dependency tracking decorator                                                                          \u2502\n\u2502 def square(num):                                                                                                \u2502\n\u2502     return num**2                                                                                               \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre>"},{"location":"tutorials/01_hello/#conclusion","title":"Conclusion","text":"<p>This was a very brief tour through the three main tools <code>mandala</code> offers: memoization, computation frames, and versioning. Later tutorials will explore these concepts in more complex situations, as well as in more realistic settings such as small machine learning projects.</p>"},{"location":"tutorials/02_ml/","title":"Random forest ML project","text":"<p>This tutorial will show you how <code>mandala</code> works in a small random forest ML project. You'll see how queriable &amp; composable memoization is a simple way to achieve the main goals of scientific data management:</p> <ul> <li> <p>iterate in the simplest way by just dumping more logic/parameters/experiments on top of the code you already ran. Memoization automatically takes care of loading past results, skipping over past computations, and merging results across compatible versions of your code.</p> </li> <li> <p>explore the interdependencies of all saved results incrementally and declaratively with computation frames, generalized dataframes that operate over memoized computation graphs. Expand the computational history of artifacts backward (to what produced them) and/or forward (to experiments that used them), perform high-level operations over slices of storage, and generate dataframes of results for further analysis.</p> </li> </ul>"},{"location":"tutorials/02_ml/#imports-setup","title":"Imports &amp; setup","text":"<pre><code># for Google Colab\ntry:\n    import google.colab\n    !pip install git+https://github.com/amakelov/mandala\nexcept:\n    pass\n</code></pre> <pre><code>from typing import Tuple\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# recommended way to import mandala functionality\nfrom mandala.imports import *\n\nnp.random.seed(0)\n</code></pre> <pre><code>@op # memoizing decorator\ndef generate_dataset(random_seed=42):\n    print(f\"Generating dataset...\")\n    X, y = load_digits(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=random_seed)\n    return X_train, X_test, y_train, y_test\n\n@op\ndef train_model(X_train, y_train):\n    print(f\"Training model...\")\n    model = RandomForestClassifier(n_estimators=1)\n    model.fit(X_train, y_train)\n    return model, round(model.score(X_train, y_train), 2)\n\n@op\ndef eval_model(model, X_test, y_test):\n    print(f\"Evaluating model...\")\n    return round(model.score(X_test, y_test), 2)\n</code></pre>"},{"location":"tutorials/02_ml/#running-and-iterating-on-the-pipeline","title":"Running and iterating on the pipeline","text":""},{"location":"tutorials/02_ml/#run-the-pipeline-once-with-default-settings","title":"Run the pipeline once with default settings","text":"<pre><code># in-memory storage for all results in this notebook; use `db_path` to persist\n# to disk\nstorage = Storage() \n\nwith storage: # block to make all @ops called inside read/write to a given storage\n    X_train, X_test, y_train, y_test = generate_dataset()\n    model, train_acc = train_model(X_train, y_train)\n    test_acc = eval_model(model, X_test, y_test)\n    print(f\"Train accuracy: {train_acc},\\nTest accuracy: {test_acc}\")\n</code></pre> <pre><code>Generating dataset...\nTraining model...\nEvaluating model...\nTrain accuracy: AtomRef(0.91, hid=8f5...),\nTest accuracy: AtomRef(0.76, hid=57f...)\n</code></pre> <p>Now all three calls are saved to the storage. <code>@op</code>s return value  references, which wrap a Python object with some storage metadata needed to make the memoization compose. To get the underlying object, call <code>storage.unwrap(ref)</code>.</p> <p>Thanks to that metadata, when we re-run memoized code, the storage recognizes step-by-step that all work has already been done, and only loads references to the results (not the Python objects themselves):</p> <pre><code>with storage: # same code, but now it only loads pointers to saved results\n    X_train, X_test, y_train, y_test = generate_dataset()\n    model, train_acc = train_model(X_train, y_train)\n    test_acc = eval_model(model, X_test, y_test)\n    print(f\"Train accuracy: {train_acc},\\nTest accuracy: {test_acc}\")\n</code></pre> <pre><code>Train accuracy: AtomRef(hid=8f5..., in_memory=False),\nTest accuracy: AtomRef(hid=57f..., in_memory=False)\n</code></pre>"},{"location":"tutorials/02_ml/#iterate-directly-on-top-of-memoized-code-and-change-memoized-functions-backward-compatibly","title":"Iterate directly on top of memoized code and change memoized functions backward-compatibly","text":"<p>This also makes it easy to iterate on a project by just adding stuff on top of already memoized code. For example, let's add a new parameter to <code>train_model</code> in a way compatible with our current results:</p> <pre><code>@op\ndef train_model(X_train, y_train, n_estimators=NewArgDefault(1)):\n    print(f\"Training model...\")\n    model = RandomForestClassifier(n_estimators=n_estimators)\n    model.fit(X_train, y_train)\n    return model, round(model.score(X_train, y_train), 2)\n\nwith storage: \n    X_train, X_test, y_train, y_test = generate_dataset()\n    for n_estimators in [1, 10, 100]:\n        print(f\"Running with {n_estimators} trees...\")\n        model, train_acc = train_model(X_train, y_train, n_estimators=n_estimators)\n        test_acc = eval_model(model, X_test, y_test)\n        print(f\"    Train accuracy={train_acc},\\n    Test accuracy={test_acc}\")\n</code></pre> <pre><code>Running with 1 trees...\n    Train accuracy=AtomRef(hid=8f5..., in_memory=False),\n    Test accuracy=AtomRef(hid=57f..., in_memory=False)\nRunning with 10 trees...\nTraining model...\nEvaluating model...\n    Train accuracy=AtomRef(1.0, hid=760...),\n    Test accuracy=AtomRef(0.94, hid=600...)\nRunning with 100 trees...\nTraining model...\nEvaluating model...\n    Train accuracy=AtomRef(1.0, hid=ab0...),\n    Test accuracy=AtomRef(0.98, hid=45f...)\n</code></pre> <p>When we add a new argument with a default value wrapped as <code>NewArgDefault(obj)</code>, this ensures backward compatibility. <code>mandala</code> will ignore this parameter when its value equals <code>obj</code>, and fall back to memoized calls that don't provide this argument. </p> <p>This is why <code>Training model...</code> got printed only two times, and why the results for <code>n_estimators=1</code> are not in memory (<code>in_memory=False</code>).</p>"},{"location":"tutorials/02_ml/#pros-and-cons-of-memoization","title":"Pros and cons of memoization","text":"<p>Composable memoization is a powerful imperative query interface: if you have the memoized code in front of you, the code becomes its own storage interface. You can just retrace it and get references to any intermediate results you want to look at. This is very flexible, because you can add control flow logic to restrict the \"query\". Retracing is cheap, because large objects are not loaded from storage, letting you narrow down your \"query\" before high-bandwidth interaction with the backend.</p> <p>However, the full memoized code may not always be in front of you! Especially in larger projects, where it's easy to lose track of what has already been computed, there's a need for a complementary storage interface based on declarative principles. We discuss this next.</p>"},{"location":"tutorials/02_ml/#querying-the-storage-with-computation-frames","title":"Querying the storage with computation frames","text":"<p>In cases when memoization-based querying is not enough, <code>mandala</code> offers the <code>ComputationFrame</code> class. A computation frame is a generalization of the familiar <code>pandas</code> dataframe in two main ways:</p> <ul> <li>the \"columns\" of a computation frame represent a computational graph, made of variables and operations on them.</li> <li>the \"rows\" of a computation frame are computations that (partially) follow the structure of the computational graph.</li> </ul> <p>It's best to illustrate this with some examples:</p> <pre><code>cf = storage.cf(train_model); cf\n</code></pre> <pre><code>ComputationFrame with:\n    5 variable(s) (10 unique refs)\n    1 operation(s) (3 unique calls)\nComputational graph:\n    var_0@output_0, var_1@output_1 = train_model(X_train=X_train, n_estimators=n_estimators, y_train=y_train)\n</code></pre> <p>We just got a computation frame (CF) corresponding to a very simple computation graph: it only has one operation, <code>train_model</code>, with its associated inputs and outputs. Output variables are appended with the name of the function output they are connected to, in order to remove ambiguity in cases when not all outputs of an operation are present in the computational graph.</p> <p>The printout also describes the overall number of <code>Ref</code>s and <code>Call</code>s represented by this CF. Much like in <code>pandas</code>, we can rename the \"columns\" of our computation frame for readability:</p> <pre><code>cf.rename(vars={'var_1': 'model', 'var_0': 'train_acc'}, inplace=True); cf\n</code></pre> <pre><code>ComputationFrame with:\n    5 variable(s) (10 unique refs)\n    1 operation(s) (3 unique calls)\nComputational graph:\n    train_acc@output_0, model@output_1 = train_model(X_train=X_train, n_estimators=n_estimators, y_train=y_train)\n</code></pre> <p>We can directly extract a dataframe from the CF:</p> <pre><code>print(cf.df().to_markdown())\n</code></pre> <pre><code>|    | y_train           | X_train                         |   n_estimators | train_model                   |   model | train_acc                               |\n|---:|:------------------|:--------------------------------|---------------:|:------------------------------|--------:|:----------------------------------------|\n|  0 | [6 0 0 ... 2 7 1] | [[ 0.  0.  3. ... 13.  4.  0.]  |            nan | Call(train_model, hid=ac0...) |    0.91 | RandomForestClassifier(n_estimators=1)  |\n|    |                   |  [ 0.  0.  9. ...  3.  0.  0.]  |                |                               |         |                                         |\n|    |                   |  [ 0.  0.  0. ...  6.  0.  0.]  |                |                               |         |                                         |\n|    |                   |  ...                            |                |                               |         |                                         |\n|    |                   |  [ 0.  0.  9. ... 16.  2.  0.]  |                |                               |         |                                         |\n|    |                   |  [ 0.  0.  1. ...  0.  0.  0.]  |                |                               |         |                                         |\n|    |                   |  [ 0.  0.  1. ...  1.  0.  0.]] |                |                               |         |                                         |\n|  1 | [6 0 0 ... 2 7 1] | [[ 0.  0.  3. ... 13.  4.  0.]  |            100 | Call(train_model, hid=255...) |    1    | RandomForestClassifier()                |\n|    |                   |  [ 0.  0.  9. ...  3.  0.  0.]  |                |                               |         |                                         |\n|    |                   |  [ 0.  0.  0. ...  6.  0.  0.]  |                |                               |         |                                         |\n|    |                   |  ...                            |                |                               |         |                                         |\n|    |                   |  [ 0.  0.  9. ... 16.  2.  0.]  |                |                               |         |                                         |\n|    |                   |  [ 0.  0.  1. ...  0.  0.  0.]  |                |                               |         |                                         |\n|    |                   |  [ 0.  0.  1. ...  1.  0.  0.]] |                |                               |         |                                         |\n|  2 | [6 0 0 ... 2 7 1] | [[ 0.  0.  3. ... 13.  4.  0.]  |             10 | Call(train_model, hid=5f7...) |    1    | RandomForestClassifier(n_estimators=10) |\n|    |                   |  [ 0.  0.  9. ...  3.  0.  0.]  |                |                               |         |                                         |\n|    |                   |  [ 0.  0.  0. ...  6.  0.  0.]  |                |                               |         |                                         |\n|    |                   |  ...                            |                |                               |         |                                         |\n|    |                   |  [ 0.  0.  9. ... 16.  2.  0.]  |                |                               |         |                                         |\n|    |                   |  [ 0.  0.  1. ...  0.  0.  0.]  |                |                               |         |                                         |\n|    |                   |  [ 0.  0.  1. ...  1.  0.  0.]] |                |                               |         |                                         |\n\n\n/home/amakelov/workspace/current/conda_envs/mandala_3.10/lib/python3.10/site-packages/tabulate/__init__.py:107: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  (len(row) &gt;= 1 and row[0] == SEPARATING_LINE)\n/home/amakelov/workspace/current/conda_envs/mandala_3.10/lib/python3.10/site-packages/tabulate/__init__.py:108: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  or (len(row) &gt;= 2 and row[1] == SEPARATING_LINE)\n</code></pre> <p>We get back a table, where each row corresponds to a call to <code>train_model</code>. Furthermore, the <code>Call</code> objects themselves (which contain metadata about the call) appear in the column dedicated to the single operation in the graph.</p> <p>We see that in the <code>n_estimators</code> column we have the values <code>[NaN, 10.0, 100.0]</code> (in some order), reflecting the fact that we made 1 call to <code>train_model</code> before introducing the <code>n_estimators</code> argument, and 2 afterwards.</p>"},{"location":"tutorials/02_ml/#exploring-storage-by-expanding-the-cf","title":"Exploring storage by expanding the CF","text":"<p>This CF gets much more interesting and useful when we can look into where the inputs to <code>train_model</code> came from, and what the outputs were used for. We can add the history of particular variables by calling <code>expand_back</code> on the CF, and  similarly <code>expand_forward</code> shows the operations that consume given variables: </p> <pre><code>print('Expanding back the inputs:')\ncf.expand_back(varnames=[\"X_train\", \"y_train\"]).draw(verbose=True)\nprint('\\nExpanding forward the outputs:')\ncf.expand_forward(varnames=['model', 'train_acc']).draw(verbose=True)\n</code></pre> <pre><code>Expanding back the inputs:\n</code></pre> <p></p> <pre><code>Expanding forward the outputs:\n</code></pre> <p></p> <p>We can perform a full expansion (until we can't go back or forward) with the <code>.expand()</code> method:</p> <pre><code>expanded_cf = cf.expand_all()\nexpanded_cf\n</code></pre> <pre><code>ComputationFrame with:\n    7 variable(s) (14 unique refs)\n    3 operation(s) (7 unique calls)\nComputational graph:\n    X_train@output_0, y_train@output_2 = generate_dataset(random_seed=random_seed)\n    train_acc@output_0, model@output_1 = train_model(X_train=X_train, n_estimators=n_estimators, y_train=y_train)\n    var_0@output_0 = eval_model(model=train_acc)\n</code></pre> <p>Note that the extracted CF represents a \"partial\" computation graph: the variables for <code>X_test</code>and <code>y_test</code> were not reached during this traversal (though they can be added). Finally, we can (again) extract a dataframe from this CF:</p> <pre><code>df = expanded_cf.df().drop(columns=['X_train', 'y_train']) # drop to avoid a bit of clutter\nprint(df.to_markdown())\n</code></pre> <pre><code>|    |   random_seed | generate_dataset                   |   n_estimators | train_model                   |   model | train_acc                               | eval_model                   |   var_0 |\n|---:|--------------:|:-----------------------------------|---------------:|:------------------------------|--------:|:----------------------------------------|:-----------------------------|--------:|\n|  0 |            42 | Call(generate_dataset, hid=c3f...) |            nan | Call(train_model, hid=ac0...) |    0.91 | RandomForestClassifier(n_estimators=1)  | Call(eval_model, hid=c5a...) |    0.76 |\n|  1 |            42 | Call(generate_dataset, hid=c3f...) |            100 | Call(train_model, hid=255...) |    1    | RandomForestClassifier()                | Call(eval_model, hid=07b...) |    0.98 |\n|  2 |            42 | Call(generate_dataset, hid=c3f...) |             10 | Call(train_model, hid=5f7...) |    1    | RandomForestClassifier(n_estimators=10) | Call(eval_model, hid=ed5...) |    0.94 |\n</code></pre>"},{"location":"tutorials/02_ml/#using-computation-frames-for-high-level-operations","title":"Using computation frames for high-level operations","text":"<p>Finally, we can illustrate the use of computation frames for easy declarative operations over the storage, even in the presence of highly heterogeneous experiments. To make this more interesting, let's train some more models:</p> <pre><code>@op\ndef train_model(X_train, y_train,\n                n_estimators=NewArgDefault(1),\n                max_depth=NewArgDefault(None) # one more backward-compatible argument\n                ):\n    print(f\"Training model...\")\n    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n    model.fit(X_train, y_train)\n    return model, round(model.score(X_train, y_train), 2)\n\nwith storage: \n    X_train, X_test, y_train, y_test = generate_dataset()\n    for n_estimators in [10, 100]:\n        for max_depth in [1, 2]:\n            model, train_acc = train_model(X_train, y_train, n_estimators=n_estimators, max_depth=max_depth)\n</code></pre> <pre><code>Training model...\nTraining model...\nTraining model...\nTraining model...\n</code></pre> <p>We left out the call to <code>eval_model</code> on purpose to illustrate how CFs handle heterogeneous and partial computations. </p> <p>As before, we build a big CF by expanding backward and forward from <code>train_model</code>, and then extract a dataframe:</p> <pre><code>cf = storage.cf(train_model).expand_all().rename(vars={'var_0': 'model', 'var_1': 'train_acc', 'var_2': 'eval_acc'})\n# use `lazy_vars` to avoid loading the large arrays which we don't need\ndf = cf.df(lazy_vars=['X_train', 'y_train'], join_how='outer').drop(columns=['X_train', 'y_train'])\nprint(df.to_markdown())\n</code></pre> <pre><code>|    |   max_depth |   n_estimators |   random_seed | generate_dataset                   | train_model                   |   train_acc | model                                                | eval_model                   |   eval_acc |\n|---:|------------:|---------------:|--------------:|:-----------------------------------|:------------------------------|------------:|:-----------------------------------------------------|:-----------------------------|-----------:|\n|  0 |         nan |            nan |            42 | Call(generate_dataset, hid=c3f...) | Call(train_model, hid=ac0...) |        0.91 | RandomForestClassifier(n_estimators=1)               | Call(eval_model, hid=c5a...) |       0.76 |\n|  1 |         nan |            100 |            42 | Call(generate_dataset, hid=c3f...) | Call(train_model, hid=255...) |        1    | RandomForestClassifier()                             | Call(eval_model, hid=07b...) |       0.98 |\n|  2 |           2 |             10 |            42 | Call(generate_dataset, hid=c3f...) | Call(train_model, hid=e61...) |        0.71 | RandomForestClassifier(max_depth=2, n_estimators=10) |                              |     nan    |\n|  3 |           1 |             10 |            42 | Call(generate_dataset, hid=c3f...) | Call(train_model, hid=f56...) |        0.59 | RandomForestClassifier(max_depth=1, n_estimators=10) |                              |     nan    |\n|  4 |           2 |            100 |            42 | Call(generate_dataset, hid=c3f...) | Call(train_model, hid=fe1...) |        0.84 | RandomForestClassifier(max_depth=2)                  |                              |     nan    |\n|  5 |         nan |             10 |            42 | Call(generate_dataset, hid=c3f...) | Call(train_model, hid=5f7...) |        1    | RandomForestClassifier(n_estimators=10)              | Call(eval_model, hid=ed5...) |       0.94 |\n|  6 |           1 |            100 |            42 | Call(generate_dataset, hid=c3f...) | Call(train_model, hid=561...) |        0.7  | RandomForestClassifier(max_depth=1)                  |                              |     nan    |\n</code></pre> <p>We see that the various missing computational paths show up as <code>NaN</code>s in the  table, under either variables (e.g. <code>max_depth</code>) or operations (e.g. <code>eval_model</code>). </p> <p>Suppose we want the models where train accuracy was above 0.8 and max depth was 2.  We can do this with familiar dataframe operations:</p> <pre><code>df.query('train_acc &gt; 0.8 and max_depth == 2').model.tolist()\n</code></pre> <pre><code>[RandomForestClassifier(max_depth=2)]\n</code></pre> <p>Similarly, we can manipulate storage by e.g. deleting calls based on this dataframe. For example, we can delete all calls to <code>train_model</code> where  <code>train_acc &lt; 0.8</code>:</p> <pre><code>storage.drop_calls(df.query('train_acc &lt; 0.8').train_model, delete_dependents=True)\n# now check that the dropped calls are gone\ncf = storage.cf(train_model).expand_all().rename(vars={'var_1': 'model', 'var_0': 'train_acc', 'var_2': 'eval_acc'})\ndf = cf.df(lazy_vars=['X_train', 'y_train']).drop(columns=['X_train', 'y_train'])\nprint(df.to_markdown())\n</code></pre> <pre>[17:32:39] INFO     Dropped 3 calls (and 3 from cache).                                              storage.py:350\n</pre> <pre><code>|    |   max_depth |   n_estimators |   random_seed | generate_dataset                   | train_model                   |   model | train_acc                               | eval_model                   |   eval_acc |\n|---:|------------:|---------------:|--------------:|:-----------------------------------|:------------------------------|--------:|:----------------------------------------|:-----------------------------|-----------:|\n|  0 |         nan |            nan |            42 | Call(generate_dataset, hid=c3f...) | Call(train_model, hid=ac0...) |    0.91 | RandomForestClassifier(n_estimators=1)  | Call(eval_model, hid=c5a...) |       0.76 |\n|  1 |         nan |            100 |            42 | Call(generate_dataset, hid=c3f...) | Call(train_model, hid=255...) |    1    | RandomForestClassifier()                | Call(eval_model, hid=07b...) |       0.98 |\n|  2 |           2 |            100 |            42 | Call(generate_dataset, hid=c3f...) | Call(train_model, hid=fe1...) |    0.84 | RandomForestClassifier(max_depth=2)     |                              |     nan    |\n|  3 |         nan |             10 |            42 | Call(generate_dataset, hid=c3f...) | Call(train_model, hid=5f7...) |    1    | RandomForestClassifier(n_estimators=10) | Call(eval_model, hid=ed5...) |       0.94 |\n</code></pre> <pre><code>cf.draw(verbose=True)\n</code></pre> <p></p>"},{"location":"tutorials/gotchas/","title":"Gotchas","text":"<p>This notebook lists some things that can go wrong when using <code>mandala</code>, and how to avoid and/or fix them.</p> <pre><code># for Google Colab\ntry:\n    import google.colab\n    !pip install git+https://github.com/amakelov/mandala\nexcept:\n    pass\n</code></pre>"},{"location":"tutorials/gotchas/#versioning-tips","title":"Versioning tips","text":""},{"location":"tutorials/gotchas/#avoiding-overhead-when-versioning-large-global-variables","title":"Avoiding overhead when versioning large global variables","text":"<p>If you use versioning (by passing the <code>deps_path=...</code> argument to a <code>Storage</code>), <code>mandala</code> will automatically track the content hashes of any global variables accessed by your <code>@op</code>s. </p> <p>This is a useful way to avoid a class of bugs, but can also lead to significant overhead if you have large global variables, because each time a <code>with storage:</code> context is entered, <code>mandala</code> will compute the hashes of all known global variables to check for changes, which can be slow.</p> <p>To overcome this, if you have a large global variable that you know will not change often, you can effectively manually pre-compute its hash so that <code>mandala</code> does not need to recompute it each time. This can be done by simply wrapping the global in a <code>Ref</code> object, and then using <code>ref.obj</code> when you want to access the underlying object in a function.</p> <pre><code>import numpy as np\nfrom mandala.imports import wrap_atom, op, Storage, track\n\nLARGE_GLOBAL = wrap_atom(np.ones((10_000, 5000)))\n\n@op\ndef test_op(x):\n    return x + LARGE_GLOBAL.obj\n\nstorage = Storage(deps_path='__main__', strict_tracing=False)\n</code></pre> <pre><code>with storage:\n    y = test_op(0)\n</code></pre> <p>You can check that now (unlike the case when you don't wrap the global), retracing the memoized code takes very little time because the hash of the large global variable is not recomputed:</p> <pre><code>with storage:\n    y = test_op(0)\n</code></pre> <p>You can also see the object reflected in the version of <code>test_op</code>:</p> <pre><code>storage.versions(test_op)\n</code></pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 ### Dependencies for version of function test_op from module __main__                                           \u2502\n\u2502 ### content_version_id=0b20075a89aec9dc391db79ff1d0aef6                                                         \u2502\n\u2502 ### semantic_version_id=4360b08a7c57f017bbebbdec2fbd92b3                                                        \u2502\n\u2502                                                                                                                 \u2502\n\u2502 ################################################################################                                \u2502\n\u2502 ### IN MODULE \"__main__\"                                                                                        \u2502\n\u2502 ################################################################################                                \u2502\n\u2502 LARGE_GLOBAL = AtomRef(array([[1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., [...]                    \u2502\n\u2502                                                                                                                 \u2502\n\u2502 @op                                                                                                             \u2502\n\u2502 def test_op(x):                                                                                                 \u2502\n\u2502     return x + LARGE_GLOBAL.obj                                                                                 \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre>"},{"location":"tutorials/gotchas/#caveats-of-hashing","title":"Caveats of hashing","text":""},{"location":"tutorials/gotchas/#if-x-y-this-doesnt-guarantee-that-x-and-y-will-have-the-same-content-hash","title":"If <code>x == y</code>, this doesn't guarantee that <code>x</code> and <code>y</code> will have the same content hash","text":"<pre><code>from mandala.utils import get_content_hash\n\nprint(f'Is 1 == True? {1 == True}')\nprint(f'Is the hash of 1 == the hash of True? {get_content_hash(1) == get_content_hash(True)}')\nprint(f'Is 23 == 23.0? {23 == 23.0}')\nprint(f'Is the hash of 23 == the hash of 23.0? {get_content_hash(23) == get_content_hash(23.0)}')\n</code></pre> <pre><code>Is 1 == True? True\nIs the hash of 1 == the hash of True? False\nIs 23 == 23.0? True\nIs the hash of 23 == the hash of 23.0? False\n</code></pre>"},{"location":"tutorials/gotchas/#hashing-numerical-values-is-sensitive-to-precision-and-type","title":"Hashing numerical values is sensitive to precision and type","text":"<p>All three of the values <code>42, 42.0, 42.000000001</code> have different content hashes:</p> <pre><code>from mandala.utils import get_content_hash\n\nprint(get_content_hash(42))\nprint(get_content_hash(42.0))\nprint(get_content_hash(42.00000000001))\n</code></pre> <pre><code>d922f805b5eead8c40ee21f14329d6c7\nca276c58eef17e13c4f274c9280abc1e\nb61bb24b62bf6b1ab95506a62843be08\n</code></pre> <p>It's possible to define custom types that will be insensitive to types and rounding errors when hashed, but this is currently not implemented.</p>"},{"location":"tutorials/gotchas/#non-deterministic-hashes-for-complex-objects","title":"Non-deterministic hashes for complex objects","text":"<p>Below we illustrate several potentially confusing behaviors that are hard to eradicate in general: - even if we set all random seeds properly, certain computations (e.g., training a <code>scikit-learn</code> model) result in objects with non-deterministic content IDs - certain objects can change their content ID after making a roundtrip through the serialization-deserialization pipeline</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_digits\nimport random\nimport numpy as np\n\nfrom mandala.utils import get_content_hash, serialize, deserialize\n\nX, y = load_digits(n_class=10, return_X_y=True)\n\ndef train_model():\n    ### set both the numpy and python random seed\n    np.random.seed(42)\n    random.seed(42)\n    ### train a model, passing the random_state explicitly\n    model = RandomForestClassifier(max_depth=2, \n                                n_estimators=100, random_state=42).fit(X, y)\n    return model\n\n### training in the exact same way will produce different content hashes\nmodel_1 = train_model()\nmodel_2 = train_model()\nprint(f'Content IDs of the two models: {get_content_hash(model_1)} and {get_content_hash(model_2)}')\n\n### a roundtrip serialization will produce a different content hash\nroundtrip_model_1 = deserialize(serialize(model_1))\nprint(f'Content IDs of the original and restored model: {get_content_hash(model_1)} and {get_content_hash(roundtrip_model_1)}')\n</code></pre> <pre><code>Content IDs of the two models: c8d1485ebe003581fb2019b73a2de97a and c8d1485ebe003581fb2019b73a2de97a\nContent IDs of the original and restored model: c8d1485ebe003581fb2019b73a2de97a and c8d1485ebe003581fb2019b73a2de97a\n</code></pre> <p>Why is this hard to get rid of in general? One pervasive issue is that some custom Python objects, e.g. many kinds of ML models and even <code>pytorch</code> tensors, create internal state related to system resources, such as memory layout. These  can be different between objects that otherwise have semantically equivalent state, leading to different content hashes. It is impossible to write down a hash function that always ignores these aspects for arbitrary classes, because  we don't know how to interpret which attributes of the object are semantically meaningful and which are contingent.</p> <p>What should you do about it? This issue does come up that often in practice. Note that this is not an issue for many kinds of objects, such as primitive Python types and nested python collections thereof, as well as some other types like numpy arrays. If you always pass as inputs to <code>@op</code>s objects like this, or <code>Ref</code>s obtained from other <code>@op</code>s, this issue will not come up. Indeed, if \"unwieldy\" objects are always results of <code>@op</code>s, a single copy of each such object will be saved and deserialized every time.</p> <p>This problem does, however, make it very difficult to detect when your <code>@op</code>s have side effects.</p>"}]}